{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3580bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-22.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fecfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< EDIT THIS >>>\n",
    "DATA_DIR = \"/home/andy/6309GP/HDFS/HDFS_v1/preprocessed\"  # e.g., \"D:/datasets/HDFS_v1/preprocessed\"\n",
    "\n",
    "# Expected files (as you said you have):\n",
    "# - anomaly_label.csv\n",
    "# - Event_occurrence_matrix.csv           (count matrix per session x event)\n",
    "# - Event_traces.csv                      (session -> sequence of event IDs)\n",
    "# - HDFS.log_templates.csv                (EventId -> EventTemplate)\n",
    "# - HDFS.npz                              (optional prebuilt sparse matrices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbe86aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (575061, 3)\n",
      "                  SessionId  \\\n",
      "0  blk_-1608999687919862906   \n",
      "1   blk_7503483334202473044   \n",
      "\n",
      "                                       EventSequence  Label  \n",
      "0  E5 E22 E5 E5 E11 E11 E9 E9 E11 E9 E26 E26 E26 ...      0  \n",
      "1  E5 E5 E22 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26 ...      0  \n",
      "Label distribution: {0: 558223, 1: 16838}\n"
     ]
    }
   ],
   "source": [
    "import ast, re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = Path(DATA_DIR)  # reuse your existing variable\n",
    "\n",
    "# ---------- 1) Load labels (prefer anomaly_label.csv if present) ----------\n",
    "labels_path = DATA_DIR / \"anomaly_label.csv\"\n",
    "if labels_path.exists():\n",
    "    y_df = pd.read_csv(labels_path, engine=\"python\")\n",
    "    # Find id + label columns\n",
    "    sid_col = next((c for c in [\"SessionId\",\"BlockId\",\"BlockID\",\"Id\",\"ID\"] if c in y_df.columns), None)\n",
    "    if sid_col is None:\n",
    "        raise ValueError(\"anomaly_label.csv: could not find a session/block id column.\")\n",
    "    if \"Label\" not in y_df.columns and \"Anomaly\" in y_df.columns:\n",
    "        y_df[\"Label\"] = y_df[\"Anomaly\"]\n",
    "    if \"Label\" not in y_df.columns:\n",
    "        raise ValueError(\"anomaly_label.csv: expected 'Label' or 'Anomaly' column.\")\n",
    "    # Normalize labels to {0,1}\n",
    "    if y_df[\"Label\"].dtype == object:\n",
    "        y_df[\"Label\"] = (\n",
    "            y_df[\"Label\"].astype(str).str.strip().str.lower()\n",
    "            .map({\"success\":0, \"normal\":0, \"fail\":1, \"failure\":1, \"anomaly\":1, \"1\":1, \"0\":0})\n",
    "            .fillna(0).astype(int)\n",
    "        )\n",
    "    else:\n",
    "        y_df[\"Label\"] = y_df[\"Label\"].astype(int)\n",
    "    y_df = y_df[[sid_col,\"Label\"]].rename(columns={sid_col:\"SessionId\"})\n",
    "else:\n",
    "    y_df = None  # we'll fallback to labels inside Event_traces.csv if present\n",
    "\n",
    "\n",
    "# ---------- 2) Load traces; auto-detect the EventSequence column ----------\n",
    "traces_path = DATA_DIR / \"Event_traces.csv\"\n",
    "df = pd.read_csv(traces_path, engine=\"python\")\n",
    "\n",
    "# helper: detect column that contains [E5,E22,...] lists\n",
    "def looks_like_event_list(s: pd.Series) -> bool:\n",
    "    s = s.astype(str).head(50)\n",
    "    # at least 60% of sampled rows match [E123(,E456)*]\n",
    "    frac = s.str.contains(r'^\\s*\\[\\s*E\\d+(?:\\s*,\\s*E\\d+)*\\s*\\]\\s*$', regex=True).mean()\n",
    "    return frac >= 0.6\n",
    "\n",
    "event_col = None\n",
    "for c in df.columns:\n",
    "    try:\n",
    "        if looks_like_event_list(df[c]):\n",
    "            event_col = c\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "if event_col is None:\n",
    "    raise ValueError(\"Could not find a column with event ID lists like [E5,E22,...] in Event_traces.csv.\")\n",
    "\n",
    "# session/block id\n",
    "sid_col = next((c for c in [\"SessionId\",\"BlockId\",\"BlockID\",\"Id\",\"ID\"] if c in df.columns), None)\n",
    "if sid_col is None:\n",
    "    raise ValueError(\"Event_traces.csv: could not find a session/block id column (e.g., BlockId/SessionId).\")\n",
    "\n",
    "# label (optional inside Event_traces.csv)\n",
    "label_in_traces = \"Label\" in df.columns\n",
    "\n",
    "# parse the [E..] list to a space-separated string for TF-IDF(ID)\n",
    "def event_list_to_space(seq_str: str) -> str:\n",
    "    s = str(seq_str)\n",
    "    # try safe literal parse first\n",
    "    try:\n",
    "        lst = ast.literal_eval(s)\n",
    "        # ensure strings like 'E5'\n",
    "        return \" \".join(str(x) for x in lst)\n",
    "    except Exception:\n",
    "        # fallback: strip brackets/spaces and split by comma\n",
    "        s2 = re.sub(r'[\\[\\]\\s]', '', s)\n",
    "        toks = [t for t in s2.split(\",\") if t]\n",
    "        return \" \".join(toks)\n",
    "\n",
    "traces = df[[sid_col, event_col]].rename(columns={sid_col:\"SessionId\", event_col:\"EventSequence\"}).copy()\n",
    "traces[\"EventSequence\"] = traces[\"EventSequence\"].astype(str).apply(event_list_to_space)\n",
    "\n",
    "# bring labels: prefer anomaly_label.csv if available, else derive from Event_traces.csv\n",
    "if y_df is not None:\n",
    "    data_df = traces.merge(y_df, on=\"SessionId\", how=\"inner\")\n",
    "else:\n",
    "    if not label_in_traces:\n",
    "        raise ValueError(\"No anomaly_label.csv and no 'Label' column in Event_traces.csv; cannot build labels.\")\n",
    "    tmp = df[[sid_col,\"Label\"]].rename(columns={sid_col:\"SessionId\"})\n",
    "    # normalize textual labels\n",
    "    tmp[\"Label\"] = (\n",
    "        tmp[\"Label\"].astype(str).str.strip().str.lower()\n",
    "        .map({\"success\":0, \"normal\":0, \"fail\":1, \"failure\":1, \"anomaly\":1, \"1\":1, \"0\":0})\n",
    "        .fillna(0).astype(int)\n",
    "    )\n",
    "    data_df = traces.merge(tmp, on=\"SessionId\", how=\"inner\")\n",
    "\n",
    "print(\"Data shape:\", data_df.shape)\n",
    "print(data_df.head(2))\n",
    "print(\"Label distribution:\", data_df[\"Label\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7384ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EventId                           EventTemplate\n",
      "0      E1  [*]Adding an already existing block[*]\n",
      "1      E2        [*]Verification succeeded for[*]\n",
      "2      E3                 [*]Served block[*]to[*]\n",
      "3      E4  [*]Got exception while serving[*]to[*]\n",
      "4      E5    [*]Receiving block[*]src:[*]dest:[*] (29, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load templates (EventId -> EventTemplate)\n",
    "templates_path = Path(DATA_DIR) / \"HDFS.log_templates.csv\"\n",
    "tpl_df = pd.read_csv(templates_path)\n",
    "\n",
    "# Normalize column names\n",
    "eid_col = None\n",
    "tpl_col = None\n",
    "for c in tpl_df.columns:\n",
    "    if c.lower() in (\"eventid\", \"eid\", \"id\", \"event_id\"):\n",
    "        eid_col = c\n",
    "    if c.lower() in (\"eventtemplate\", \"template\", \"logtemplate\", \"log_template\"):\n",
    "        tpl_col = c\n",
    "if eid_col is None or tpl_col is None:\n",
    "    raise ValueError(\"HDFS.log_templates.csv must have columns for EventId and EventTemplate.\")\n",
    "tpl_df = tpl_df[[eid_col, tpl_col]].rename(columns={eid_col: \"EventId\", tpl_col: \"EventTemplate\"})\n",
    "\n",
    "# Make EventId strings like 'E1' consistent if needed\n",
    "tpl_df[\"EventId\"] = tpl_df[\"EventId\"].astype(str)\n",
    "print(tpl_df.head(), tpl_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "604b953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  SessionId  \\\n",
      "0  blk_-1608999687919862906   \n",
      "1   blk_7503483334202473044   \n",
      "2  blk_-3544583377289625738   \n",
      "3  blk_-9073992586687739851   \n",
      "4   blk_7854771516489510256   \n",
      "\n",
      "                                       EventSequence  Label  \n",
      "0  E5 E22 E5 E5 E11 E11 E9 E9 E11 E9 E26 E26 E26 ...      0  \n",
      "1  E5 E5 E22 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26 ...      0  \n",
      "2  E5 E22 E5 E5 E11 E9 E11 E9 E11 E9 E3 E26 E26 E...      1  \n",
      "3  E5 E22 E5 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26 ...      0  \n",
      "4  E5 E5 E22 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26 ...      0   (575061, 3) Label\n",
      "0    558223\n",
      "1     16838\n",
      "Name: count, dtype: int64\n",
      " DataFrame saved as sequence_hdfs_20251024_183457.bin\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Merge step\n",
    "data_df = traces.merge(y_df, on=\"SessionId\", how=\"inner\")\n",
    "print(data_df.head(), data_df.shape, data_df[\"Label\"].value_counts(dropna=False))\n",
    "\n",
    "# Generate timestamped filename\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"sequence_hdfs_{timestamp}.bin\"\n",
    "\n",
    "# Save as binary (structured NumPy array)\n",
    "data_df.to_feather(filename)\n",
    "\n",
    "print(f\" DataFrame saved as {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f162fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/andyVenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((575061, 29), 29)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only tokens that look like event IDs (E#, if you prefer), but simplest is just split on space\n",
    "def identity_tokenizer(s: str):\n",
    "    # For pre-tokenized input (space-separated)\n",
    "    return s.split()\n",
    "\n",
    "tfidf_id = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False, preprocessor=None)\n",
    "X_id = tfidf_id.fit_transform(data_df[\"EventSequence\"].astype(str))\n",
    "\n",
    "feature_names_id = np.array(tfidf_id.get_feature_names_out())\n",
    "X_id.shape, len(feature_names_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1f9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9986    0.9993    167468\n",
      "           1     0.9544    0.9996    0.9765      5051\n",
      "\n",
      "    accuracy                         0.9986    172519\n",
      "   macro avg     0.9772    0.9991    0.9879    172519\n",
      "weighted avg     0.9987    0.9986    0.9986    172519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############ TODO : redo for 10 fold cross validation ###########################\n",
    "# Train/test split (same 70/30 as paper)\n",
    "Xtr_id, Xte_id, ytr, yte = train_test_split(X_id, data_df[\"Label\"].values, test_size=0.30, random_state=42, stratify=data_df[\"Label\"].values)\n",
    "\n",
    "# Quick baseline model to verify features work (Logistic Regression)\n",
    "clf_id = LogisticRegression(max_iter=200, class_weight=\"balanced\", n_jobs=None)\n",
    "clf_id.fit(Xtr_id, ytr)\n",
    "yp_id = clf_id.predict(Xte_id)\n",
    "print(classification_report(yte, yp_id, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e5a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/andy/6309GP/HDFS/HDFS_v1/preprocessed/tfidf_outputs/tfidf_id_splits.npz and feature names JSON\n"
     ]
    }
   ],
   "source": [
    "# Save TF-IDF(ID) artifacts for reuse\n",
    "out_dir = Path(DATA_DIR) / \"tfidf_outputs\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "np.savez_compressed(out_dir / \"tfidf_id_splits.npz\",\n",
    "                    Xtr=Xtr_id, Xte=Xte_id, ytr=ytr, yte=yte)\n",
    "with open(out_dir / \"tfidf_id_feature_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(feature_names_id.tolist(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved: {out_dir/'tfidf_id_splits.npz'} and feature names JSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3416030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  SessionId                                SessionTemplateText\n",
      "0  blk_-1608999687919862906  [*]Receiving block[*]src:[*]dest:[*] . [*]BLOC...\n",
      "1   blk_7503483334202473044  [*]Receiving block[*]src:[*]dest:[*] . [*]Rece...\n",
      "2  blk_-3544583377289625738  [*]Receiving block[*]src:[*]dest:[*] . [*]BLOC...\n"
     ]
    }
   ],
   "source": [
    "# Build mapping EventId -> Template text\n",
    "eid2tpl = dict(zip(tpl_df[\"EventId\"].astype(str), tpl_df[\"EventTemplate\"].astype(str)))\n",
    "\n",
    "def session_text_from_ids(seq: str) -> str:\n",
    "    # seq is \"E1 E2 E1 ...\": map each to template text and concatenate\n",
    "    words = []\n",
    "    for eid in seq.split():\n",
    "        tpl = eid2tpl.get(str(eid))\n",
    "        if tpl:\n",
    "            words.append(tpl)\n",
    "        else:\n",
    "            # fallback: if EventId isn't in template table (shouldn't happen), include raw ID\n",
    "            words.append(str(eid))\n",
    "    # repeating templates naturally weights them by frequency\n",
    "    return \" . \".join(words)\n",
    "\n",
    "data_df[\"SessionTemplateText\"] = data_df[\"EventSequence\"].astype(str).apply(session_text_from_ids)\n",
    "print(data_df[[\"SessionId\", \"SessionTemplateText\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78a5c94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((575061, 74), 74)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF over template text tokens\n",
    "# Strip numbers by token pattern if you want to mimic common preprocessing:\n",
    "# token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z_]+\\b\"  -> keeps alphabetic tokens\n",
    "tfidf_text = TfidfVectorizer(\n",
    "    max_features=None,\n",
    "    ngram_range=(1, 1),\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z_]+\\b\"  # ignore pure numbers and punctuation\n",
    ")\n",
    "\n",
    "X_txt = tfidf_text.fit_transform(data_df[\"SessionTemplateText\"])\n",
    "feature_names_txt = np.array(tfidf_text.get_feature_names_out())\n",
    "X_txt.shape, len(feature_names_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c841db3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9965    0.9982    167468\n",
      "           1     0.8948    1.0000    0.9445      5051\n",
      "\n",
      "    accuracy                         0.9966    172519\n",
      "   macro avg     0.9474    0.9982    0.9713    172519\n",
      "weighted avg     0.9969    0.9966    0.9966    172519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train/test split (70/30)\n",
    "Xtr_txt, Xte_txt, ytr, yte = train_test_split(X_txt, data_df[\"Label\"].values, test_size=0.30, random_state=42, stratify=data_df[\"Label\"].values)\n",
    "\n",
    "# Quick baseline classifier\n",
    "clf_txt = LogisticRegression(max_iter=200, class_weight=\"balanced\", n_jobs=None)\n",
    "clf_txt.fit(Xtr_txt, ytr)\n",
    "yp_txt = clf_txt.predict(Xte_txt)\n",
    "print(classification_report(yte, yp_txt, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73afd051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: HDFS_v1\\preprocessed\\tfidf_outputs\\tfidf_text_splits.npz and feature names JSON\n"
     ]
    }
   ],
   "source": [
    "# Save TF-IDF(Text) artifacts\n",
    "np.savez_compressed(out_dir / \"tfidf_text_splits.npz\",\n",
    "                    Xtr=Xtr_txt, Xte=Xte_txt, ytr=ytr, yte=yte)\n",
    "with open(out_dir / \"tfidf_text_feature_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(feature_names_txt.tolist(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved: {out_dir/'tfidf_text_splits.npz'} and feature names JSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efefc9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 143870, label=0, top TF-IDF(ID) terms: [('E23', 0.4268472363412375), ('E21', 0.42561380306813124), ('E9', 0.3558869640127482), ('E26', 0.3558869640127482), ('E11', 0.3558708728946056), ('E5', 0.35206623822982014), ('E2', 0.34492935215939297), ('E22', 0.11735541274327338)]\n",
      "Sample 71818, label=0, top TF-IDF(ID) terms: [('E4', 0.5121531324880011), ('E23', 0.37370254829949945), ('E21', 0.3726226838466611), ('E9', 0.311577196793221), ('E26', 0.311577196793221), ('E11', 0.3115631091024902), ('E5', 0.3082321711262576), ('E3', 0.2498043776813618), ('E22', 0.10274405704208588)]\n",
      "Sample 11032, label=0, top TF-IDF(ID) terms: [('E23', 0.4547562409076315), ('E21', 0.4534421607615443), ('E9', 0.3791562979995132), ('E26', 0.3791562979995132), ('E11', 0.37913915477876464), ('E5', 0.37508575765942254), ('E22', 0.12502858588647417)]\n"
     ]
    }
   ],
   "source": [
    "# Show a couple of anomalous vs normal sessions’ most-weighted TF-IDF(ID) features\n",
    "def top_k_nonzero_features(row_vec, feature_names, k=10):\n",
    "    row = row_vec.tocsr()\n",
    "    start, end = row.indptr[0], row.indptr[1]\n",
    "    cols = row.indices[start:end]\n",
    "    vals = row.data[start:end]\n",
    "    if vals.size == 0:\n",
    "        return []\n",
    "    top = np.argsort(vals)[::-1][:k]\n",
    "    return [(feature_names[cols[i]], float(vals[i])) for i in top]\n",
    "\n",
    "\n",
    "sample_idx = np.random.choice(Xte_id.shape[0], 3, replace=False)\n",
    "for i in sample_idx:\n",
    "    feats = top_k_nonzero_features(Xte_id[i], feature_names_id, k=10)\n",
    "    print(f\"Sample {i}, label={yte[i]}, top TF-IDF(ID) terms:\", feats[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa7d946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 145574, label=0, top TF-IDF(Text) terms: [('block', 0.6447014105224141), ('to', 0.2932640544507517), ('namesystem', 0.22564549368284495), ('is', 0.19550936963383445), ('of', 0.19550936963383445), ('size', 0.19550936963383445), ('added', 0.19550936963383445), ('serving', 0.16068367184645865), ('while', 0.16068367184645865), ('got', 0.1606816773796902)]\n",
      "Sample 6804, label=0, top TF-IDF(Text) terms: [('block', 0.685125583603246), ('size', 0.3196434308349457), ('namesystem', 0.21080787187792185), ('to', 0.15982171541747284), ('blockmap', 0.15982171541747284), ('added', 0.15982171541747284), ('is', 0.15982171541747284), ('updated', 0.15982171541747284), ('of', 0.15982171541747284), ('from', 0.15982171541747284)]\n",
      "Sample 116817, label=0, top TF-IDF(Text) terms: [('block', 0.685125583603246), ('size', 0.3196434308349457), ('namesystem', 0.21080787187792185), ('to', 0.15982171541747284), ('blockmap', 0.15982171541747284), ('added', 0.15982171541747284), ('is', 0.15982171541747284), ('updated', 0.15982171541747284), ('of', 0.15982171541747284), ('from', 0.15982171541747284)]\n"
     ]
    }
   ],
   "source": [
    "# Peek template-text features for another random sample\n",
    "sample_idx = np.random.choice(Xte_txt.shape[0], 3, replace=False)\n",
    "for i in sample_idx:\n",
    "    feats = top_k_nonzero_features(Xte_txt[i], feature_names_txt, k=10)\n",
    "    print(f\"Sample {i}, label={yte[i]}, top TF-IDF(Text) terms:\", feats[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andyVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
