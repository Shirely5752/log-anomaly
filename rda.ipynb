{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae281ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.linalg import inv, eigh\n",
    "\n",
    "# ---------- 1) Load & clean ----------\n",
    "PATH = \"HDFS_structured.csv\"\n",
    "\n",
    "df = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2974542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LineId  Label              Timestamp                   BlockId  \\\n",
      "0       1      0              Timestamp                       NaN   \n",
      "1       2      0  \"2008-11-09 20:35:18\"  blk_-1608999687919862906   \n",
      "2       3      0  \"2008-11-09 20:35:18\"  blk_-1608999687919862906   \n",
      "3       4      0  \"2008-11-09 20:35:19\"  blk_-1608999687919862906   \n",
      "4       5      0  \"2008-11-09 20:35:19\"  blk_-1608999687919862906   \n",
      "\n",
      "                                             Content  \\\n",
      "0                                    BlockId Message   \n",
      "1  \"dfs.DataNode$DataXceiver: Receiving block blk...   \n",
      "2  \"dfs.FSNamesystem: BLOCK* NameSystem.allocateB...   \n",
      "3  \"dfs.DataNode$DataXceiver: Receiving block blk...   \n",
      "4  \"dfs.DataNode$DataXceiver: Receiving block blk...   \n",
      "\n",
      "                                       EventTemplate   EventId  \n",
      "0                                    BlockId Message  1ea271b9  \n",
      "1  <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*...  e8ffe641  \n",
      "2  \"dfs.FSNamesystem: BLOCK* NameSystem.allocateB...  0303acc9  \n",
      "3  <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*...  e8ffe641  \n",
      "4  <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*...  e8ffe641  \n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3344fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the spurious header-like row (where Timestamp literally equals \"Timestamp\" or BlockId is NaN)\n",
    "mask_junk = (df['Timestamp'].astype(str).str.strip('\"') == 'Timestamp') | df['BlockId'].isna()\n",
    "df = df.loc[~mask_junk].copy()\n",
    "\n",
    "# Normalize columns\n",
    "# --- Normalize block id column safely ---\n",
    "if 'block_id' not in df.columns and 'BlockId' in df.columns:\n",
    "    df['block_id'] = df['BlockId']\n",
    "elif 'block_id' in df.columns and df['block_id'].isna().all() and 'BlockId' in df.columns:\n",
    "    df['block_id'] = df['BlockId']\n",
    "# if both exist and are valid, keep as is\n",
    "\n",
    "# Drop rows missing block_id or EventId\n",
    "df = df.dropna(subset=['block_id', 'EventId'])\n",
    "\n",
    "# Parse Timestamp like \"2008-11-09 20:35:18\"\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'].astype(str).str.strip('\"'), errors='coerce')\n",
    "# If some rows didn't parse, keep them but they'll get hour 0 (goes to 'night')\n",
    "df['hour'] = df['Timestamp'].dt.hour.fillna(0).astype(int)\n",
    "df['hour_bin'] = pd.cut(df['hour'], bins=[-1,5,11,17,23],\n",
    "                        labels=['night','morning','afternoon','evening']).astype(str)\n",
    "\n",
    "# ---------- 2) Build session-event matrix X ----------\n",
    "# Count of EventId per block (use presence/absence for Jaccard)\n",
    "event_counts = (\n",
    "    df.groupby(['block_id', 'EventId'])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    "      .astype(np.int32)\n",
    ")\n",
    "X_counts = event_counts\n",
    "X_bin = (X_counts > 0).astype(int)\n",
    "\n",
    "# ---------- 3) Build predictors Y (label + hour_bin) ----------\n",
    "# Label may already be 0/1 in this file; if it's strings, map to 0/1\n",
    "# We'll take the first label per block_id (HDFS labels are per-session)\n",
    "lab_per_block = (\n",
    "    df.groupby('block_id')['Label'].first()\n",
    "      .pipe(lambda s: s.astype(str).str.lower().map({'normal':0,'anomaly':1})\n",
    "            if s.dtype == 'O' or s.dtype.name.startswith('str') else s.astype(int))\n",
    ")\n",
    "\n",
    "hourbin_per_block = df.groupby('block_id')['hour_bin'].first().astype(str)\n",
    "\n",
    "Y_df = pd.DataFrame({\n",
    "    'label': lab_per_block.reindex(X_counts.index).fillna(0).astype(int),\n",
    "    'hour_bin': hourbin_per_block.reindex(X_counts.index).fillna('unknown').astype(str),\n",
    "})\n",
    "Y = pd.get_dummies(Y_df, drop_first=True).astype(float)  # one-hot for hour_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e3346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6000 sessions; distance matrix shape: (6000, 6000), dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4) Distances (non-parametric) with stratified sampling ----------\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Choose a manageable cap for sessions (adjust to your RAM; 4000–8000 is typical)\n",
    "MAX_N = 6000\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# We'll stratify by the block-level label you already built (Y_df['label'])\n",
    "block_labels = Y_df['label'].reindex(X_bin.index)\n",
    "\n",
    "# Compute per-class quotas\n",
    "counts = block_labels.value_counts(dropna=False)\n",
    "total = counts.sum()\n",
    "quotas = (counts / total * MAX_N).round().astype(int).clip(lower=1)\n",
    "\n",
    "# Sample indices per class\n",
    "sample_idx = []\n",
    "for cls, q in quotas.items():\n",
    "    idx_cls = block_labels[block_labels == cls].index\n",
    "    q = min(q, len(idx_cls))\n",
    "    # random sample without replacement\n",
    "    pick = rng.choice(idx_cls.to_numpy(), size=q, replace=False)\n",
    "    sample_idx.append(pick)\n",
    "sample_idx = np.concatenate(sample_idx)\n",
    "\n",
    "# Subset X and Y consistently\n",
    "X_bin_s = X_bin.loc[sample_idx]\n",
    "Y_s = Y.loc[sample_idx]\n",
    "\n",
    "# Convert to boolean for efficient Jaccard in SciPy\n",
    "X_bool = X_bin_s.values.astype(bool)\n",
    "\n",
    "# Condensed vector -> square matrix; cast to float32 to save memory\n",
    "D = squareform(pdist(X_bool, metric='jaccard')).astype(np.float32)\n",
    "\n",
    "print(f\"Using {len(sample_idx)} sessions; distance matrix shape: {D.shape}, dtype: {D.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9974ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5) db-RDA (CAP) ----------\n",
    "def gower_center(D):\n",
    "    D2 = D**2\n",
    "    n = D.shape[0]\n",
    "    J = np.eye(n) - np.ones((n, n))/n\n",
    "    return -0.5 * J @ D2 @ J\n",
    "\n",
    "def hat_matrix(Y):\n",
    "    XtX = Y.T @ Y\n",
    "    return Y @ inv(XtX) @ Y.T\n",
    "\n",
    "def dbrda(D, Y_df, add_intercept=True, n_perm=999, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = D.shape[0]\n",
    "    G = gower_center(D)\n",
    "\n",
    "    # Scale continuous columns only (heuristic: non-binary)\n",
    "    Yc = Y_df.copy()\n",
    "    non_binary = [c for c in Yc.columns if not set(Yc[c].unique()).issubset({0.0,1.0})]\n",
    "    if non_binary:\n",
    "        Yc[non_binary] = StandardScaler().fit_transform(Yc[non_binary])\n",
    "\n",
    "    Ymat = Yc.values\n",
    "    if add_intercept:\n",
    "        Ymat = np.column_stack([np.ones(n), Ymat])\n",
    "\n",
    "    H = hat_matrix(Ymat)\n",
    "    num = np.trace(H @ G @ H)\n",
    "    den = np.trace(G)\n",
    "    R2 = num / den\n",
    "\n",
    "    # Permutation p-value (shuffle rows of Y)\n",
    "    greater = 0\n",
    "    for _ in range(n_perm):\n",
    "        perm = rng.permutation(n)\n",
    "        Hp = hat_matrix(Ymat[perm, :])\n",
    "        R2p = np.trace(Hp @ G @ Hp) / den\n",
    "        if R2p >= R2:\n",
    "            greater += 1\n",
    "    pval = (greater + 1) / (n_perm + 1)\n",
    "\n",
    "    # Canonical axes (for plotting if you like)\n",
    "    HGH = H @ G @ H\n",
    "    eigvals, eigvecs = eigh(HGH)\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    scores2d = eigvecs[:, :2] * np.sqrt(np.maximum(eigvals[:2], 0))\n",
    "\n",
    "    return {\"R2\": float(R2), \"pval\": float(pval), \"scores2d\": scores2d, \"eigvals\": eigvals}\n",
    "\n",
    "# res = dbrda(D, Y_s, add_intercept=True, n_perm=999, random_state=7)\n",
    "# print(f\"db-RDA on sampled HDFS logs → R^2={res['R2']:.4f}, p={res['pval']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9982655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique contribution of label                = 0.0162\n",
      "Unique contribution of hour_bin_evening     = 0.1017\n",
      "Unique contribution of hour_bin_morning     = 0.0257\n",
      "Unique contribution of hour_bin_night       = 0.0886\n"
     ]
    }
   ],
   "source": [
    "def partial_dbrda(D, Y_full, target_col, n_perm=999, random_state=42):\n",
    "    \"\"\"Unique effect of one predictor, controlling for all others.\"\"\"\n",
    "    others = Y_full.drop(columns=[target_col])\n",
    "    res_all = dbrda(D, Y_full, n_perm=199, random_state=random_state)\n",
    "    res_others = dbrda(D, others, n_perm=199, random_state=random_state)\n",
    "    unique_r2 = res_all['R2'] - res_others['R2']\n",
    "    return unique_r2\n",
    "\n",
    "for col in Y_s.columns:\n",
    "    uniq = partial_dbrda(D, Y_s, col)\n",
    "    print(f\"Unique contribution of {col:<20} = {uniq:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
