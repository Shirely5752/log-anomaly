{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ccd3fe16-89c9-4b22-a97b-3944a937faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.363779 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.527847 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.675872 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.823719 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.50.982731 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.131467 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.293532 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.428563 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.601412 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "- 1117838571 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.42.51.749199 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\Shirley\\Downloads\\BGL\\BGL.log\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f17668-aeff-4e69-949f-ca8324e12718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: logparser 0.8.4\n",
      "Uninstalling logparser-0.8.4:\n",
      "  Successfully uninstalled logparser-0.8.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall logparser -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe3614-1ed8-46fb-8590-0e7be69c13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/logpai/logparser.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6a363f-a7e6-4082-bee4-531d0ea11059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logparser import Drain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71991df0-6f39-4fa9-9a11-af2c19110881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drain parser loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from logparser.Drain import LogParser\n",
    "print(\"Drain parser loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffb91e31-806a-4ced-9a11-2c8c95b4eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc74b84-95f9-4d9b-9339-15445e13d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cb6b6a4-31ed-4769-94f9-f13d1fd984df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing done.\n",
      "   Total lines processed: 4747963\n",
      "   Normal (label=0): 4399503\n",
      "   Abnormal (label=1): 348460\n",
      "   Saved cleaned file to: C:\\Users\\Shirley\\Downloads\\BGL\\BGL_clean_for_parsing.log\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 🔧 Step 0 — Import dependencies\n",
    "# ===============================\n",
    "import re\n",
    "\n",
    "# ===============================\n",
    "# 📂 Step 1 — Input and Output paths\n",
    "# ===============================\n",
    "input_path = r\"C:\\Users\\Shirley\\Downloads\\BGL\\BGL.log\"\n",
    "output_path = r\"C:\\Users\\Shirley\\Downloads\\BGL\\BGL_clean_for_parsing.log\"\n",
    "\n",
    "# ===============================\n",
    "# 🧹 Step 2 — Preprocess logs\n",
    "# ===============================\n",
    "total = 0\n",
    "normal = 0\n",
    "abnormal = 0\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as infile, \\\n",
    "     open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        total += 1\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # skip empty\n",
    "\n",
    "        parts = line.split()\n",
    "        if len(parts) < 7:\n",
    "            continue  # skip malformed lines\n",
    "\n",
    "        # ✅ Step 2a: Label\n",
    "        first_col = parts[0]\n",
    "        if first_col == \"-\":\n",
    "            label = 0\n",
    "            normal += 1\n",
    "        else:\n",
    "            label = 1\n",
    "            abnormal += 1\n",
    "\n",
    "        # ✅ Step 2b: Timestamp\n",
    "        timestamp = parts[4]  # e.g. 2005-06-03-15.42.50.363779\n",
    "\n",
    "        # ✅ Step 2c: Message\n",
    "        # Keep everything after the 5th field as message\n",
    "        message = \" \".join(parts[5:]).strip()\n",
    "\n",
    "        # ✅ Output cleaned line\n",
    "        outfile.write(f\"{label}\\t{timestamp}\\t{message}\\n\")\n",
    "\n",
    "print(f\"✅ Preprocessing done.\")\n",
    "print(f\"   Total lines processed: {total}\")\n",
    "print(f\"   Normal (label=0): {normal}\")\n",
    "print(f\"   Abnormal (label=1): {abnormal}\")\n",
    "print(f\"   Saved cleaned file to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca81795e-41f6-4f04-b74b-3c3d5e7c3974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4747963\n",
      "Label\n",
      "0    4399503\n",
      "1     348460\n",
      "Name: count, dtype: int64\n",
      "Timestamp range: 2005-06-03-15.42.50.363779 → 2006-01-04-08.00.05.233639\n",
      "   Label                   Timestamp  \\\n",
      "0      0  2005-06-03-15.42.50.363779   \n",
      "1      0  2005-06-03-15.42.50.527847   \n",
      "2      0  2005-06-03-15.42.50.675872   \n",
      "3      0  2005-06-03-15.42.50.823719   \n",
      "4      0  2005-06-03-15.42.50.982731   \n",
      "5      0  2005-06-03-15.42.51.131467   \n",
      "6      0  2005-06-03-15.42.51.293532   \n",
      "7      0  2005-06-03-15.42.51.428563   \n",
      "8      0  2005-06-03-15.42.51.601412   \n",
      "9      0  2005-06-03-15.42.51.749199   \n",
      "\n",
      "                                             Message  \n",
      "0  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n",
      "1  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n",
      "2  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n",
      "3  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n",
      "4  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n",
      "5  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n",
      "6  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n",
      "7  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n",
      "8  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n",
      "9  R02-M1-N0-C:J12-U11 RAS KERNEL INFO instructio...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Shirley\\Downloads\\BGL\\BGL_clean_for_parsing.log\",\n",
    "                 sep=\"\\t\", names=[\"Label\", \"Timestamp\", \"Message\"])\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "print(df[\"Label\"].value_counts())\n",
    "print(\"Timestamp range:\", df[\"Timestamp\"].min(), \"→\", df[\"Timestamp\"].max())\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03f20ad8-e586-4708-a0f1-337af8b7a74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 4747963\n",
      "⚠️ Unexpected labels: [0 1]\n",
      "✅ All timestamps are valid\n",
      "✅ All messages are non-empty\n",
      "\n",
      "Label distribution:\n",
      "Label\n",
      "0    4399503\n",
      "1     348460\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Timestamp range:\n",
      "2005-06-03-15.42.50.363779 → 2006-01-04-08.00.05.233639\n",
      "\n",
      "Sample head:\n",
      " Label                  Timestamp                                                                      Message\n",
      "     0 2005-06-03-15.42.50.363779 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "     0 2005-06-03-15.42.50.527847 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "     0 2005-06-03-15.42.50.675872 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "     0 2005-06-03-15.42.50.823719 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "     0 2005-06-03-15.42.50.982731 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "     0 2005-06-03-15.42.51.131467 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "     0 2005-06-03-15.42.51.293532 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "     0 2005-06-03-15.42.51.428563 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "     0 2005-06-03-15.42.51.601412 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "     0 2005-06-03-15.42.51.749199 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ===============================\n",
    "# 📂 Step 0 — Input file\n",
    "# ===============================\n",
    "file_path = r\"C:\\Users\\Shirley\\Downloads\\BGL\\BGL_clean_for_parsing.log\"\n",
    "\n",
    "# ===============================\n",
    "# 🧾 Step 1 — Load preprocessed file\n",
    "# ===============================\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Timestamp\", \"Message\"])\n",
    "\n",
    "# ===============================\n",
    "# ✅ Step 2 — Basic checks\n",
    "# ===============================\n",
    "print(\"Total rows:\", len(df))\n",
    "\n",
    "# Label check\n",
    "if set(df[\"Label\"].unique()) <= {\"0\", \"1\"}:\n",
    "    print(\"✅ Labels are valid (0/1)\")\n",
    "else:\n",
    "    print(\"⚠️ Unexpected labels:\", df[\"Label\"].unique())\n",
    "\n",
    "# Timestamp check\n",
    "def is_valid_timestamp(ts):\n",
    "    try:\n",
    "        datetime.strptime(ts, \"%Y-%m-%d-%H.%M.%S.%f\")\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "invalid_ts = df[~df[\"Timestamp\"].apply(is_valid_timestamp)]\n",
    "if len(invalid_ts) == 0:\n",
    "    print(\"✅ All timestamps are valid\")\n",
    "else:\n",
    "    print(f\"⚠️ {len(invalid_ts)} invalid timestamps found\")\n",
    "\n",
    "# Message check\n",
    "empty_msg = df[df[\"Message\"].str.strip() == \"\"]\n",
    "if len(empty_msg) == 0:\n",
    "    print(\"✅ All messages are non-empty\")\n",
    "else:\n",
    "    print(f\"⚠️ {len(empty_msg)} empty messages found\")\n",
    "\n",
    "# ===============================\n",
    "# 📊 Step 3 — Additional statistics\n",
    "# ===============================\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df[\"Label\"].value_counts())\n",
    "\n",
    "print(\"\\nTimestamp range:\")\n",
    "print(df[\"Timestamp\"].min(), \"→\", df[\"Timestamp\"].max())\n",
    "\n",
    "print(\"\\nSample head:\")\n",
    "print(df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f87b60b5-d0c6-45f6-8964-9bc2ed235859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Label\n",
      "0    4295303\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_path, sep=\"\\t\", names=[\"Label\", \"Timestamp\", \"Message\"])\n",
    "print(df[\"Label\"].unique())  # 会显示 [0] 表示全是正常\n",
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "789c7d2d-3141-457c-acd7-bba0ef875a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t2005-06-03-15.42.50.363779\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "0\t2005-06-03-15.42.50.527847\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "0\t2005-06-03-15.42.50.675872\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "0\t2005-06-03-15.42.50.823719\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "0\t2005-06-03-15.42.50.982731\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "0\t2005-06-03-15.42.51.131467\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "0\t2005-06-03-15.42.51.293532\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "0\t2005-06-03-15.42.51.428563\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "0\t2005-06-03-15.42.51.601412\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "0\t2005-06-03-15.42.51.749199\tR02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n"
     ]
    }
   ],
   "source": [
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "575eba80-f6fc-4a06-a6c8-f3c92e10e160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Loading cleaned log file...\n",
      "✅ Loaded 4,747,963 log entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing logs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4747963/4747963 [16:29<00:00, 4796.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved structured logs: C:\\Users\\Shirley\\Downloads\\BGL\\result_new\\BGL_structured.csv\n",
      "✅ Saved template summary: C:\\Users\\Shirley\\Downloads\\BGL\\result_new\\BGL_templates.csv\n",
      "🕒 Total parsing time: 0:17:55.183756\n",
      "✅ Saved merged structured log: C:\\Users\\Shirley\\Downloads\\BGL\\result_new\\BGL_structured_labeled.csv\n",
      "\n",
      "Top 10 templates:\n",
      " EventId                                                                                                                                                                                                                   EventTemplate  Occurrences\n",
      "82e013db                                                                                                                                                                                         <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>      1037941\n",
      "f4efd1ca                                                                                                                                     <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>       656744\n",
      "73d6de92                                                                                                                                                                                 <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>       585369\n",
      "023c9167                                                                                                                                                         <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>       533248\n",
      "71585e7e                                                                                                                                                                         <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>       532500\n",
      "76c99562                                                                                                                                                 <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>       346391\n",
      "244e4fb7                                                                                                     <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>       330928\n",
      "e733e90e                                                                                                                                                                                  <*> RAS KERNEL INFO CE sym <*> at <*> mask <*>       201206\n",
      "e0e27083 <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>       146912\n",
      "8d29c08a                                                                                                                                                                    <*> RAS KERNEL INFO instruction cache parity error corrected       105924\n",
      "\n",
      "Structured log head:\n",
      " LineId                                                                      Content                                                EventTemplate  EventId  Label                  Timestamp\n",
      "      1 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.50.363779\n",
      "      2 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.50.527847\n",
      "      3 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.50.675872\n",
      "      4 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.50.823719\n",
      "      5 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.50.982731\n",
      "      6 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.51.131467\n",
      "      7 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.51.293532\n",
      "      8 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.51.428563\n",
      "      9 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.51.601412\n",
      "     10 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected <*> RAS KERNEL INFO instruction cache parity error corrected 8d29c08a      0 2005-06-03-15.42.51.749199\n",
      "\n",
      "Total structured rows: 4,747,963\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 🔧 Step 0 — Import dependencies\n",
    "# ===============================\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# ===============================\n",
    "# 🧠 Step 1 — Define SimpleDrain Parser\n",
    "# ===============================\n",
    "class SimpleDrain:\n",
    "    def __init__(self, st=0.5, max_templates=None, verbose=True):\n",
    "        \"\"\"\n",
    "        st: similarity threshold (0~1), larger = stricter merging\n",
    "        max_templates: optional limit on number of templates\n",
    "        \"\"\"\n",
    "        self.st = st\n",
    "        self.max_templates = max_templates\n",
    "        self.templates = []\n",
    "        self.template_counts = []\n",
    "        self.log_to_template = []\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def tokenize(self, line):\n",
    "        \"\"\"Split message into tokens\"\"\"\n",
    "        return re.split(r\"\\s+\", line.strip())\n",
    "\n",
    "    def similarity(self, seq1, seq2):\n",
    "        \"\"\"Compute token-level similarity\"\"\"\n",
    "        if not seq1 or not seq2:\n",
    "            return 0.0\n",
    "        same = sum(1 for i in range(min(len(seq1), len(seq2)))\n",
    "                   if seq1[i] == seq2[i] or seq1[i] == \"<*>\" or seq2[i] == \"<*>\")\n",
    "        return same / max(len(seq1), len(seq2))\n",
    "\n",
    "    def merge_template(self, temp, tokens):\n",
    "        \"\"\"Merge two templates\"\"\"\n",
    "        L = max(len(temp), len(tokens))\n",
    "        merged = []\n",
    "        for i in range(L):\n",
    "            t = temp[i] if i < len(temp) else \"<*>\"\n",
    "            s = tokens[i] if i < len(tokens) else \"<*>\"\n",
    "            merged.append(t if t == s else \"<*>\")\n",
    "        return merged\n",
    "\n",
    "    def add_log(self, tokens):\n",
    "        \"\"\"Add new log to template pool\"\"\"\n",
    "        best_sim, best_idx = -1.0, None\n",
    "        for idx, temp in enumerate(self.templates):\n",
    "            sim = self.similarity(temp, tokens)\n",
    "            if sim > best_sim:\n",
    "                best_sim, best_idx = sim, idx\n",
    "\n",
    "        if best_sim >= self.st and best_idx is not None:\n",
    "            new_temp = self.merge_template(self.templates[best_idx], tokens)\n",
    "            self.templates[best_idx] = new_temp\n",
    "            self.template_counts[best_idx] += 1\n",
    "            return best_idx\n",
    "        else:\n",
    "            if self.max_templates and len(self.templates) >= self.max_templates:\n",
    "                self.template_counts[best_idx] += 1\n",
    "                new_temp = self.merge_template(self.templates[best_idx], tokens)\n",
    "                self.templates[best_idx] = new_temp\n",
    "                return best_idx\n",
    "            else:\n",
    "                self.templates.append(tokens)\n",
    "                self.template_counts.append(1)\n",
    "                return len(self.templates) - 1\n",
    "\n",
    "    def parse_lines(self, lines):\n",
    "        \"\"\"Main parsing\"\"\"\n",
    "        self.log_to_template = []\n",
    "        iterator = tqdm(lines, desc=\"Parsing logs\") if self.verbose else lines\n",
    "        for line in iterator:\n",
    "            tokens = self.tokenize(line)\n",
    "            idx = self.add_log(tokens)\n",
    "            self.log_to_template.append(idx)\n",
    "\n",
    "    def save_results(self, lines, output_dir, base_name=\"BGL\"):\n",
    "        \"\"\"Save structured results\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        records = []\n",
    "        for i, line in enumerate(lines):\n",
    "            tidx = self.log_to_template[i]\n",
    "            template_tokens = self.templates[tidx]\n",
    "            template_str = \" \".join(template_tokens)\n",
    "            event_id = hashlib.md5(template_str.encode(\"utf-8\")).hexdigest()[:8]\n",
    "            records.append({\n",
    "                \"LineId\": i + 1,\n",
    "                \"Content\": line,\n",
    "                \"EventTemplate\": template_str,\n",
    "                \"EventId\": event_id\n",
    "            })\n",
    "        df_struct = pd.DataFrame(records)\n",
    "        struct_path = os.path.join(output_dir, f\"{base_name}_structured.csv\")\n",
    "        df_struct.to_csv(struct_path, index=False, encoding=\"utf-8\")\n",
    "        if self.verbose:\n",
    "            print(f\"✅ Saved structured logs: {struct_path}\")\n",
    "\n",
    "        # Template summary\n",
    "        temps = []\n",
    "        for i, temp in enumerate(self.templates):\n",
    "            temp_str = \" \".join(temp)\n",
    "            event_id = hashlib.md5(temp_str.encode(\"utf-8\")).hexdigest()[:8]\n",
    "            occ = self.template_counts[i]\n",
    "            temps.append({\"EventId\": event_id, \"EventTemplate\": temp_str, \"Occurrences\": occ})\n",
    "        df_temp = pd.DataFrame(temps).sort_values(by=\"Occurrences\", ascending=False)\n",
    "        temp_path = os.path.join(output_dir, f\"{base_name}_templates.csv\")\n",
    "        df_temp.to_csv(temp_path, index=False, encoding=\"utf-8\")\n",
    "        if self.verbose:\n",
    "            print(f\"✅ Saved template summary: {temp_path}\")\n",
    "\n",
    "# ===============================\n",
    "# 📂 Step 2 — Paths\n",
    "# ===============================\n",
    "input_file = r\"C:\\Users\\Shirley\\Downloads\\BGL\\BGL_clean_for_parsing.log\"\n",
    "output_dir = r\"C:\\Users\\Shirley\\Downloads\\BGL\\result_new\"\n",
    "base_name = \"BGL\"\n",
    "\n",
    "# ===============================\n",
    "# 🔍 Step 3 — Load cleaned data\n",
    "# ===============================\n",
    "print(\"🔍 Loading cleaned log file...\")\n",
    "data = pd.read_csv(input_file, sep=\"\\t\", header=None, names=[\"Label\", \"Timestamp\", \"Message\"], dtype=str)\n",
    "print(f\"✅ Loaded {len(data):,} log entries\")\n",
    "\n",
    "logs = data[\"Message\"].tolist()\n",
    "\n",
    "# ===============================\n",
    "# ⚙️ Step 4 — Run SimpleDrain\n",
    "# ===============================\n",
    "parser = SimpleDrain(st=0.5, max_templates=None, verbose=True)\n",
    "start_time = datetime.now()\n",
    "parser.parse_lines(logs)\n",
    "parser.save_results(logs, output_dir, base_name=base_name)\n",
    "print(f\"🕒 Total parsing time: {datetime.now() - start_time}\")\n",
    "\n",
    "# ===============================\n",
    "# 🧾 Step 5 — Merge label + timestamp\n",
    "# ===============================\n",
    "structured_path = os.path.join(output_dir, f\"{base_name}_structured.csv\")\n",
    "structured = pd.read_csv(structured_path)\n",
    "structured[\"Label\"] = data[\"Label\"].astype(int).values\n",
    "structured[\"Timestamp\"] = data[\"Timestamp\"].values\n",
    "\n",
    "merged_path = os.path.join(output_dir, f\"{base_name}_structured_labeled.csv\")\n",
    "structured.to_csv(merged_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Saved merged structured log: {merged_path}\")\n",
    "\n",
    "# ===============================\n",
    "# 🔎 Step 6 — Quick sanity check\n",
    "# ===============================\n",
    "df_temp = pd.read_csv(os.path.join(output_dir, f\"{base_name}_templates.csv\"))\n",
    "print(\"\\nTop 10 templates:\")\n",
    "print(df_temp.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nStructured log head:\")\n",
    "print(structured.head(10).to_string(index=False))\n",
    "print(f\"\\nTotal structured rows: {len(structured):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f96223d3-dec5-4a06-a574-9e9c39efd67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    4399503\n",
      "1     348460\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Shirley\\Downloads\\BGL\\result_new\\BGL_structured_labeled.csv\")\n",
    "print(df['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "794ee89f-33b1-4a56-91ce-25c15258b9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Loading structured log file...\n",
      "✅ Loaded 4,747,963 structured log entries\n",
      "🔧 Generating TF-IDF sparse matrix (max_features=5000)...\n",
      "✅ TF-IDF sparse matrix saved to: C:\\Users\\Shirley\\Downloads\\BGL\\result_new\\BGL_tfidf_matrix.npz\n",
      "✅ Metadata (labels + timestamps) saved to: C:\\Users\\Shirley\\Downloads\\BGL\\result_new\\BGL_metadata.csv\n",
      "\n",
      "🧩 Sanity check results:\n",
      "TF-IDF shape: (4747963, 144)\n",
      "Non-zero entries: 4,195,276 (0.6136% density)\n",
      "Label distribution:\n",
      "Label\n",
      "0    4399503\n",
      "1     348460\n",
      "Name: count, dtype: int64\n",
      "Timestamp range: 2005-06-03-15.42.50.363779 → 2006-01-04-08.00.05.233639\n",
      "✅ Row count matches between TF-IDF matrix and metadata.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 🚀 Step 0 — Import dependencies\n",
    "# ============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "# ============================================\n",
    "# 📂 Step 1 — Input / Output paths\n",
    "# ============================================\n",
    "structured_path = r\"C:\\Users\\Shirley\\Downloads\\BGL\\result_new\\BGL_structured_labeled.csv\"\n",
    "output_dir = r\"C:\\Users\\Shirley\\Downloads\\BGL\\result_new\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "tfidf_npz_path = os.path.join(output_dir, \"BGL_tfidf_matrix.npz\")\n",
    "meta_csv_path = os.path.join(output_dir, \"BGL_metadata.csv\")\n",
    "\n",
    "print(\"🔍 Loading structured log file...\")\n",
    "df = pd.read_csv(structured_path)\n",
    "print(f\"✅ Loaded {len(df):,} structured log entries\")\n",
    "\n",
    "# ============================================\n",
    "# 🧠 Step 2 — Select text field for TF-IDF\n",
    "# ============================================\n",
    "# 如果 replication 需要按模板分析，使用 EventTemplate；\n",
    "# 如果按原始日志分析，改为 df[\"Content\"]\n",
    "if \"EventTemplate\" in df.columns:\n",
    "    texts = df[\"EventTemplate\"].astype(str).values\n",
    "else:\n",
    "    texts = df[\"Content\"].astype(str).values\n",
    "\n",
    "# ============================================\n",
    "# ⚙️ Step 3 — TF-IDF vectorization\n",
    "# ============================================\n",
    "print(\"🔧 Generating TF-IDF sparse matrix (max_features=5000)...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# ============================================\n",
    "# 💾 Step 4 — Save sparse TF-IDF matrix + metadata\n",
    "# ============================================\n",
    "sparse.save_npz(tfidf_npz_path, tfidf_matrix)\n",
    "meta_df = df[[\"Label\", \"Timestamp\"]].copy()\n",
    "meta_df.to_csv(meta_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ TF-IDF sparse matrix saved to: {tfidf_npz_path}\")\n",
    "print(f\"✅ Metadata (labels + timestamps) saved to: {meta_csv_path}\")\n",
    "\n",
    "# ============================================\n",
    "# 🔎 Step 5 — Quick sanity check\n",
    "# ============================================\n",
    "print(\"\\n🧩 Sanity check results:\")\n",
    "print(f\"TF-IDF shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {tfidf_matrix.nnz:,} \"\n",
    "      f\"({tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4%} density)\")\n",
    "print(f\"Label distribution:\\n{meta_df['Label'].value_counts()}\")\n",
    "print(f\"Timestamp range: {meta_df['Timestamp'].iloc[0]} → {meta_df['Timestamp'].iloc[-1]}\")\n",
    "\n",
    "# Verify alignment\n",
    "if len(meta_df) == tfidf_matrix.shape[0]:\n",
    "    print(\"✅ Row count matches between TF-IDF matrix and metadata.\")\n",
    "else:\n",
    "    print(\"❌ Row count mismatch — check preprocessing or parsing outputs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9228de6-a48b-4071-863a-a869019a991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in BGL folder:\n",
      "- anaconda_projects\n",
      "- BGL.log\n",
      "- BGL_clean.log\n",
      "- BGL_clean_for_parsing.log\n",
      "- result_custom\n",
      "- result_full\n",
      "- result_new\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "bgl_dir = r\"C:\\Users\\Shirley\\Downloads\\BGL\"\n",
    "\n",
    "print(\"Files in BGL folder:\")\n",
    "for f in os.listdir(bgl_dir):\n",
    "    print(\"-\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4341809f-c48f-443f-97c5-b28a615c7808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\System32\\bgl_analysis.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_path = os.path.abspath(\"bgl_analysis.ipynb\")\n",
    "print(notebook_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384802da-4f61-4a75-958d-33f62e59c058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Windows\\\\System32'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.abspath(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d270cfd-9ded-4cb8-bf15-3736c79cb4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HDFS data strat ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd61808-76a8-426e-82ed-3c6bc5035a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Paths configured:\n",
      "  Log file:      D:\\data anomaly\\HDFS.log\n",
      "  Label file:    D:\\data anomaly\\anomaly_label.csv\n",
      "  Output folder: D:\\data anomaly\\result_new\n",
      "✅ HDFS.log found\n",
      "✅ anomaly_label.csv found\n",
      "\n",
      "🔍 Previewing first 5 lines of HDFS.log:\n",
      "081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010\n",
      "081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906\n",
      "081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010\n",
      "081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010\n",
      "081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating\n",
      "\n",
      "🧩 Format check results:\n",
      "  Timestamp pattern found: False\n",
      "  BlockId pattern found:    True\n",
      "⚠️ Warning: Log format not matched perfectly — please verify manually.\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Import dependencies ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === Step 2: Define paths ===\n",
    "base_dir = r\"D:\\data anomaly\"  # 你的HDFS文件目录\n",
    "hdfs_log_path = os.path.join(base_dir, \"HDFS.log\")\n",
    "label_file_path = os.path.join(base_dir, \"anomaly_label.csv\")  # 有时叫 anomaly_label.txt\n",
    "output_dir = os.path.join(base_dir, \"result_new\")\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"📁 Paths configured:\")\n",
    "print(f\"  Log file:      {hdfs_log_path}\")\n",
    "print(f\"  Label file:    {label_file_path}\")\n",
    "print(f\"  Output folder: {output_dir}\")\n",
    "\n",
    "# === Step 3: Check that files exist ===\n",
    "if not os.path.exists(hdfs_log_path):\n",
    "    raise FileNotFoundError(f\"❌ Log file not found: {hdfs_log_path}\")\n",
    "else:\n",
    "    print(\"✅ HDFS.log found\")\n",
    "\n",
    "if not os.path.exists(label_file_path):\n",
    "    print(\"⚠️  Label file not found (this may be okay if you’ll add later).\")\n",
    "else:\n",
    "    print(\"✅ anomaly_label.csv found\")\n",
    "\n",
    "# === Step 4: Preview raw log structure ===\n",
    "print(\"\\n🔍 Previewing first 5 lines of HDFS.log:\")\n",
    "with open(hdfs_log_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline().strip())\n",
    "\n",
    "# === Step 5: Quick format inspection ===\n",
    "# 用正则检测是否存在时间戳和 blockid\n",
    "sample_lines = []\n",
    "with open(hdfs_log_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i in range(100):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        sample_lines.append(line.strip())\n",
    "\n",
    "timestamp_pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d+\")\n",
    "block_pattern = re.compile(r\"blk_-?\\d+\")\n",
    "\n",
    "timestamp_found = any(timestamp_pattern.search(l) for l in sample_lines)\n",
    "block_found = any(block_pattern.search(l) for l in sample_lines)\n",
    "\n",
    "print(\"\\n🧩 Format check results:\")\n",
    "print(f\"  Timestamp pattern found: {timestamp_found}\")\n",
    "print(f\"  BlockId pattern found:    {block_found}\")\n",
    "\n",
    "if timestamp_found and block_found:\n",
    "    print(\"✅ HDFS log format looks correct — ready for preprocessing.\")\n",
    "else:\n",
    "    print(\"⚠️ Warning: Log format not matched perfectly — please verify manually.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d770116-8353-4bde-874d-3c694943f22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010\n",
      "081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906\n",
      "081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010\n",
      "081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010\n",
      "081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating\n",
      "081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating\n",
      "081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6\n",
      "081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102\n",
      "081109 203519 147 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating\n",
      "081109 203519 147 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"D:\\data anomaly\\HDFS.log\" \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b99538-356b-4371-9eed-192a10a9b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlockId,Label\n",
      "blk_-1608999687919862906,Normal\n",
      "blk_7503483334202473044,Normal\n",
      "blk_-3544583377289625738,Anomaly\n",
      "blk_-9073992586687739851,Normal\n",
      "blk_7854771516489510256,Normal\n",
      "blk_1717858812220360316,Normal\n",
      "blk_-2519617320378473615,Normal\n",
      "blk_7063315473424667801,Normal\n",
      "blk_8586544123689943463,Normal\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"D:\\data anomaly\\anomaly_label.csv\" \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db77cd8a-a1fa-4798-989a-2c3878a11449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Log file: D:\\data anomaly\\HDFS.log\n",
      "📁 Label file: D:\\data anomaly\\anomaly_label.csv\n",
      "📁 Output folder: D:\\data anomaly\\result_new\n",
      "✅ Loaded label file — shape: (575061, 2)\n",
      "✅ Label mapping example:\n",
      "                    BlockId  Label\n",
      "0  blk_-1608999687919862906      0\n",
      "1   blk_7503483334202473044      0\n",
      "2  blk_-3544583377289625738      1\n",
      "3  blk_-9073992586687739851      0\n",
      "4   blk_7854771516489510256      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing HDFS log: 11175629it [12:26, 14969.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total parsed records: 11,175,629\n",
      "✅ Cleaned log saved to: D:\\data anomaly\\result_new\\HDFS_clean_for_parsing.log\n",
      "\n",
      "🧩 Sanity check results:\n",
      "Total rows: 11175629\n",
      "Label distribution:\n",
      "Label\n",
      "0    10887379\n",
      "1      288250\n",
      "Name: count, dtype: int64\n",
      "Timestamp range: 2008-11-09 20:35:18 → 2008-11-11 11:16:28\n",
      "\n",
      "Data sample:\n",
      " Label           Timestamp                  BlockId                                                                                                                              Message\n",
      "     0 2008-11-09 20:35:18 blk_-1608999687919862906              dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010\n",
      "     0 2008-11-09 20:35:18 blk_-1608999687919862906 dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906\n",
      "     0 2008-11-09 20:35:19 blk_-1608999687919862906                  dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010\n",
      "     0 2008-11-09 20:35:19 blk_-1608999687919862906              dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010\n",
      "     0 2008-11-09 20:35:19 blk_-1608999687919862906                                       dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating\n",
      "     0 2008-11-09 20:35:19 blk_-1608999687919862906                                       dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating\n",
      "     0 2008-11-09 20:35:19 blk_-1608999687919862906                                dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6\n",
      "     0 2008-11-09 20:35:19 blk_-1608999687919862906                              dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102\n",
      "     0 2008-11-09 20:35:19 blk_-1608999687919862906                                       dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating\n",
      "     0 2008-11-09 20:35:19 blk_-1608999687919862906                              dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 🧹 Step 1 — Import dependencies\n",
    "# ==========================================\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 📂 Step 2 — Input and output configuration\n",
    "# ==========================================\n",
    "log_path = r\"D:\\data anomaly\\HDFS.log\"\n",
    "label_path = r\"D:\\data anomaly\\anomaly_label.csv\"\n",
    "output_dir = r\"D:\\data anomaly\\result_new\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"HDFS_clean_for_parsing.log\")\n",
    "\n",
    "print(f\"📁 Log file: {log_path}\")\n",
    "print(f\"📁 Label file: {label_path}\")\n",
    "print(f\"📁 Output folder: {output_dir}\")\n",
    "\n",
    "# ==========================================\n",
    "# 🔍 Step 3 — Load and preview input files\n",
    "# ==========================================\n",
    "assert os.path.exists(log_path), \"❌ HDFS.log not found\"\n",
    "assert os.path.exists(label_path), \"❌ anomaly_label.csv not found\"\n",
    "\n",
    "# Load anomaly labels\n",
    "label_df = pd.read_csv(label_path)\n",
    "print(\"✅ Loaded label file — shape:\", label_df.shape)\n",
    "\n",
    "# Normalize label values\n",
    "label_df[\"Label\"] = label_df[\"Label\"].map({\"Normal\": 0, \"Anomaly\": 1})\n",
    "label_df = label_df.dropna(subset=[\"BlockId\"])\n",
    "label_map = dict(zip(label_df[\"BlockId\"], label_df[\"Label\"]))\n",
    "\n",
    "print(\"✅ Label mapping example:\")\n",
    "print(label_df.head())\n",
    "\n",
    "# ==========================================\n",
    "# 🧩 Step 4 — Parse HDFS.log\n",
    "# ==========================================\n",
    "block_pattern = re.compile(r\"(blk_-?\\d+)\")\n",
    "parsed_records = []\n",
    "\n",
    "with open(log_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for line in tqdm(f, desc=\"Parsing HDFS log\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        parts = line.split()\n",
    "        if len(parts) < 5:\n",
    "            continue\n",
    "        \n",
    "        # 1️⃣ Timestamp conversion\n",
    "        try:\n",
    "            ts_raw = parts[0] + \" \" + parts[1]\n",
    "            ts = datetime.strptime(ts_raw, \"%y%m%d %H%M%S\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        except:\n",
    "            ts = None\n",
    "        \n",
    "        # 2️⃣ Extract block ID\n",
    "        match = block_pattern.search(line)\n",
    "        block_id = match.group(1) if match else None\n",
    "        if not block_id:\n",
    "            continue\n",
    "        \n",
    "        # 3️⃣ Extract message (after INFO)\n",
    "        msg_index = line.find(\"INFO\")\n",
    "        message = line[msg_index + 5 :] if msg_index != -1 else line\n",
    "        \n",
    "        # 4️⃣ Lookup label\n",
    "        label = label_map.get(block_id, 0)  # 未在label文件中出现的默认Normal(0)\n",
    "        \n",
    "        parsed_records.append((label, ts, block_id, message))\n",
    "\n",
    "print(f\"✅ Total parsed records: {len(parsed_records):,}\")\n",
    "\n",
    "# ==========================================\n",
    "# 💾 Step 5 — Save cleaned logs\n",
    "# ==========================================\n",
    "clean_df = pd.DataFrame(parsed_records, columns=[\"Label\", \"Timestamp\", \"BlockId\", \"Message\"])\n",
    "clean_df.to_csv(output_file, index=False, sep=\" \", encoding=\"utf-8\")\n",
    "print(f\"✅ Cleaned log saved to: {output_file}\")\n",
    "\n",
    "# ==========================================\n",
    "# 🔎 Step 6 — Sanity check\n",
    "# ==========================================\n",
    "print(\"\\n🧩 Sanity check results:\")\n",
    "print(\"Total rows:\", len(clean_df))\n",
    "print(\"Label distribution:\")\n",
    "print(clean_df[\"Label\"].value_counts())\n",
    "print(\"Timestamp range:\", clean_df[\"Timestamp\"].min(), \"→\", clean_df[\"Timestamp\"].max())\n",
    "print(\"\\nData sample:\")\n",
    "print(clean_df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c645ea7f-06d8-4e03-a6b7-a63d5c13ab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cleaned rows: 11,175,630\n",
      "Unique BlockIds: 575062\n",
      "Label distribution (sample): [(0, 10887380), (1, 288250)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing logs: 100%|█████████████████████████████████████████████████████| 11175630/11175630 [14:30<00:00, 12841.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved structured logs: D:\\data anomaly\\result_new\\HDFS_structured.csv\n",
      "✅ Saved templates summary: D:\\data anomaly\\result_new\\HDFS_templates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating per block: 100%|█████████████████████████████████████████████████| 575062/575062 [02:41<00:00, 3567.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved block-level event traces: D:\\data anomaly\\result_new\\HDFS_event_traces.csv\n",
      "Total unique blocks: 575062\n",
      "\n",
      "Top 10 templates (by occurrence):\n",
      " EventId                                                                                       EventTemplate  Occurrences\n",
      "73d6de92                                                     <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>      4924388\n",
      "76c99562                     <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>      1961207\n",
      "2036c23b                              \"dfs.DataNode$PacketResponder: Received block <*> of size <*> from <*>      1706514\n",
      "5efcf91c \"dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: <*> is added to <*> size <*>      1476706\n",
      "0303acc9                                         \"dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: <*> <*>       575061\n",
      "023c9167                             <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>       359104\n",
      "a9930d49                                               \"dfs.DataBlockScanner: Verification succeeded for <*>       120036\n",
      "e8ffe641 <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>        41468\n",
      "f4efd1ca         <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>        11140\n",
      "8969bb5b                                                                    \"dfs.FSDataset: Reopen Block <*>            5\n",
      "\n",
      "Structured head:\n",
      "  LineId  Label             Timestamp                  BlockId                                                                                                                                                  Content                                               EventTemplate  EventId _ts_parsed\n",
      "       1      0             Timestamp                                                                                                                                                                   BlockId Message                                             BlockId Message 1ea271b9        NaT\n",
      "10365305      0 \"2008-11-11 09:44:52\" blk_-1000002529962039464                                  \"dfs.DataNode$DataXceiver: Receiving block blk_-1000002529962039464 src: /10.251.123.1:41333 dest: /10.251.123.1:50010\"             <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> 73d6de92        NaT\n",
      "10365352      0 \"2008-11-11 09:44:52\" blk_-1000002529962039464                                  \"dfs.DataNode$DataXceiver: Receiving block blk_-1000002529962039464 src: /10.251.123.1:53174 dest: /10.251.123.1:50010\"             <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> 73d6de92        NaT\n",
      "10365361      0 \"2008-11-11 09:44:52\" blk_-1000002529962039464                              \"dfs.DataNode$DataXceiver: Receiving block blk_-1000002529962039464 src: /10.251.202.181:32980 dest: /10.251.202.181:50010\"             <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> 73d6de92        NaT\n",
      "10365423      0 \"2008-11-11 09:44:52\" blk_-1000002529962039464 \"dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_001261_0/part-01261. blk_-1000002529962039464\" \"dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: <*> <*> 0303acc9        NaT\n",
      "10365599      0 \"2008-11-11 09:44:54\" blk_-1000002529962039464                                                         \"dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1000002529962039464 terminating\"             <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> 73d6de92        NaT\n",
      "\n",
      "Structured rows: 11,175,630, Templates: 11, Blocks: 575,062\n",
      "\n",
      "Parsing done — outputs saved. Next: representation (e.g., TF-IDF/Messages Count) and model construction per replication plan.\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# HDFS parsing -> structured + template + block-level traces\n",
    "# Ready to run in one Jupyter cell\n",
    "# ===============================\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------------\n",
    "# Config: update paths if needed\n",
    "# -------------------------------\n",
    "input_file = r\"D:\\data anomaly\\result_new\\HDFS_clean_for_parsing.log\"\n",
    "output_dir = r\"D:\\data anomaly\\result_new\"\n",
    "base_name = \"HDFS\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Simple Drain-like parser class\n",
    "# (lightweight, deterministic; tune st to adjust granularity)\n",
    "# -------------------------------\n",
    "class SimpleDrain:\n",
    "    def __init__(self, st=0.5, verbose=True):\n",
    "        \"\"\"\n",
    "        st: similarity threshold (0..1). Higher => more templates (more strict).\n",
    "        \"\"\"\n",
    "        self.st = st\n",
    "        self.templates = []         # template tokens lists\n",
    "        self.counts = []            # occurrences per template\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _tokenize(self, message):\n",
    "        # simple whitespace tokenization, preserving tokens\n",
    "        return re.split(r\"\\s+\", message.strip())\n",
    "\n",
    "    def _sim(self, t1, t2):\n",
    "        # similarity: proportion of same tokens at same positions normalized by max length\n",
    "        if not t1 or not t2:\n",
    "            return 0.0\n",
    "        same = 0\n",
    "        min_len = min(len(t1), len(t2))\n",
    "        for i in range(min_len):\n",
    "            if t1[i] == t2[i] or t1[i] == \"<*>\" or t2[i] == \"<*>\":\n",
    "                same += 1\n",
    "        return same / max(len(t1), len(t2))\n",
    "\n",
    "    def _merge(self, templ, tokens):\n",
    "        # merge template and tokens into generalized template (use \"<*>\" for differing tokens)\n",
    "        L = max(len(templ), len(tokens))\n",
    "        out = []\n",
    "        for i in range(L):\n",
    "            a = templ[i] if i < len(templ) else \"<*>\"\n",
    "            b = tokens[i] if i < len(tokens) else \"<*>\"\n",
    "            out.append(a if a == b else \"<*>\")\n",
    "        return out\n",
    "\n",
    "    def parse_messages(self, messages):\n",
    "        \"\"\"\n",
    "        messages: list of strings (message only, e.g., 'dfs.DataNode$... Received block ...')\n",
    "        returns: list of template_index for each message\n",
    "        \"\"\"\n",
    "        mapping = []\n",
    "        for msg in tqdm(messages, desc=\"Parsing logs\"):\n",
    "            tokens = self._tokenize(msg)\n",
    "            best_idx, best_sim = None, -1.0\n",
    "            for idx, templ in enumerate(self.templates):\n",
    "                # quick length check: allow different lengths but it's OK\n",
    "                sim = self._sim(templ, tokens)\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                    best_idx = idx\n",
    "            if best_sim >= self.st and best_idx is not None:\n",
    "                # merge into chosen template\n",
    "                self.templates[best_idx] = self._merge(self.templates[best_idx], tokens)\n",
    "                self.counts[best_idx] += 1\n",
    "                mapping.append(best_idx)\n",
    "            else:\n",
    "                # new template\n",
    "                self.templates.append(tokens)\n",
    "                self.counts.append(1)\n",
    "                mapping.append(len(self.templates) - 1)\n",
    "        return mapping\n",
    "\n",
    "    def get_template_strings_and_ids(self):\n",
    "        templ_strs = [\" \".join(t) for t in self.templates]\n",
    "        event_ids = [hashlib.md5(s.encode(\"utf-8\")).hexdigest()[:8] for s in templ_strs]\n",
    "        return templ_strs, event_ids, self.counts\n",
    "\n",
    "# -------------------------------\n",
    "# Step A: Load cleaned log (label, timestamp, blockid, message)\n",
    "# The cleaned file format expected per line (space-separated):\n",
    "# Label<space>Timestamp<space>BlockId<space>Message...\n",
    "# Example line:\n",
    "# 0 2008-11-09 20:35:18 blk_-160899... dfs.DataNode$DataXceiver: Receiving block ...\n",
    "#\n",
    "# We parse robustly using regex to extract first token (label), a timestamp pattern,\n",
    "# a blockid (blk_-...), and remaining message (the rest of line).\n",
    "# -------------------------------\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"Input cleaned file not found: {input_file}\")\n",
    "\n",
    "label_ts_blk_msg = []  # tuples: (label:int, timestamp:str, blockid:str, message:str)\n",
    "line_re = re.compile(r'^\\s*(\\d+)\\s+(\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2})\\s+(blk_-?\\d+)\\s+(.*)$')\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fin:\n",
    "    for raw in fin:\n",
    "        raw = raw.rstrip(\"\\n\")\n",
    "        if not raw:\n",
    "            continue\n",
    "        m = line_re.match(raw)\n",
    "        if m:\n",
    "            lab = int(m.group(1))\n",
    "            ts = m.group(2)\n",
    "            blk = m.group(3)\n",
    "            msg = m.group(4)\n",
    "            label_ts_blk_msg.append((lab, ts, blk, msg))\n",
    "        else:\n",
    "            # fallback: more tolerant splitting (try first token, next two tokens as timestamp parts)\n",
    "            parts = raw.split()\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "            # label = parts[0]; timestamp could be parts[1] + \" \" + parts[2]; block = parts[3]; rest = ...\n",
    "            label = int(parts[0]) if parts[0].isdigit() else 0\n",
    "            # try assemble timestamp from next two tokens if looks like YYYY-MM-DD / HH:MM:SS\n",
    "            timestamp = parts[1]\n",
    "            if len(parts) > 2 and re.match(r'\\d{2}:\\d{2}:\\d{2}', parts[2]):\n",
    "                timestamp = parts[1] + \" \" + parts[2]\n",
    "                blk = parts[3] if len(parts) > 3 else \"\"\n",
    "                msg = \" \".join(parts[4:]) if len(parts) > 4 else \"\"\n",
    "            else:\n",
    "                # worst-case: put second token as timestamp and the rest as message; attempt to find blk\n",
    "                timestamp = parts[1]\n",
    "                blk_search = re.search(r'(blk_-?\\d+)', raw)\n",
    "                blk = blk_search.group(1) if blk_search else \"\"\n",
    "                # message: everything after block if present else everything after timestamp\n",
    "                if blk:\n",
    "                    msg = raw.split(blk, 1)[1].strip()\n",
    "                else:\n",
    "                    msg = \" \".join(parts[2:])\n",
    "            label_ts_blk_msg.append((label, timestamp, blk, msg))\n",
    "\n",
    "print(f\"Loaded cleaned rows: {len(label_ts_blk_msg):,}\")\n",
    "\n",
    "# Basic sanity checks\n",
    "if len(label_ts_blk_msg) == 0:\n",
    "    raise RuntimeError(\"No parsed cleaned rows — check input format.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step B: Prepare lists for parsing (we parse the message field)\n",
    "# -------------------------------\n",
    "labels = [t[0] for t in label_ts_blk_msg]\n",
    "timestamps = [t[1] for t in label_ts_blk_msg]\n",
    "blockids = [t[2] for t in label_ts_blk_msg]\n",
    "messages = [t[3] for t in label_ts_blk_msg]\n",
    "\n",
    "# quick stats\n",
    "from collections import Counter\n",
    "blk_count = Counter(blockids)\n",
    "print(\"Unique BlockIds:\", len(blk_count))\n",
    "print(\"Label distribution (sample):\", Counter(labels).most_common())\n",
    "\n",
    "# -------------------------------\n",
    "# Step C: Run parser on messages (this is the core: message -> templates)\n",
    "# Tune st for HDFS (0.45-0.6). Paper used Drain; this is a simplified Drain-like impl.\n",
    "# -------------------------------\n",
    "parser = SimpleDrain(st=0.5, verbose=True)\n",
    "mapping = parser.parse_messages(messages)  # list of template indices per message\n",
    "\n",
    "# derive template strings and event ids\n",
    "templ_strs, event_ids, templ_counts = parser.get_template_strings_and_ids()\n",
    "\n",
    "# -------------------------------\n",
    "# Step D: Build structured dataframe (one row per original message)\n",
    "# -------------------------------\n",
    "records = []\n",
    "for i, (lab, ts, blk, msg) in enumerate(label_ts_blk_msg):\n",
    "    tidx = mapping[i]\n",
    "    template = templ_strs[tidx]\n",
    "    eid = event_ids[tidx]\n",
    "    records.append({\n",
    "        \"LineId\": i+1,\n",
    "        \"Label\": lab,\n",
    "        \"Timestamp\": ts,\n",
    "        \"BlockId\": blk,\n",
    "        \"Content\": msg,\n",
    "        \"EventTemplate\": template,\n",
    "        \"EventId\": eid\n",
    "    })\n",
    "\n",
    "df_struct = pd.DataFrame(records)\n",
    "struct_path = os.path.join(output_dir, f\"{base_name}_structured.csv\")\n",
    "df_struct.to_csv(struct_path, index=False, encoding=\"utf-8\")\n",
    "print(\"✅ Saved structured logs:\", struct_path)\n",
    "\n",
    "# -------------------------------\n",
    "# Step E: Templates summary (unique templates + occurrences)\n",
    "# -------------------------------\n",
    "df_temps = pd.DataFrame({\n",
    "    \"EventId\": event_ids,\n",
    "    \"EventTemplate\": templ_strs,\n",
    "    \"Occurrences\": templ_counts\n",
    "})\n",
    "df_temps = df_temps.sort_values(by=\"Occurrences\", ascending=False).reset_index(drop=True)\n",
    "temps_path = os.path.join(output_dir, f\"{base_name}_templates.csv\")\n",
    "df_temps.to_csv(temps_path, index=False, encoding=\"utf-8\")\n",
    "print(\"✅ Saved templates summary:\", temps_path)\n",
    "\n",
    "# -------------------------------\n",
    "# Step F: Aggregate per BlockId -> event sequence traces (ordered by timestamp)\n",
    "# This produces one row per BlockId: sequence of EventIds (space-separated) + label (block-level)\n",
    "# Note: for replication of RQ1 you will need to work with these traces (HDFS uses block traces).\n",
    "# -------------------------------\n",
    "# Convert timestamp strings to sortable datetimes when possible\n",
    "def safe_parse_ts(s):\n",
    "    try:\n",
    "        # expected format 'YYYY-MM-DD HH:MM:SS' or 'YYYY-MM-DD HH:MM:SS.something'\n",
    "        return pd.to_datetime(s, errors='coerce')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "df_struct[\"_ts_parsed\"] = df_struct[\"Timestamp\"].apply(safe_parse_ts)\n",
    "# sort by BlockId then timestamp\n",
    "df_struct = df_struct.sort_values(by=[\"BlockId\", \"_ts_parsed\", \"LineId\"])\n",
    "\n",
    "# group into sequences:\n",
    "grouped = df_struct.groupby(\"BlockId\")  # preserves original order after sort\n",
    "block_rows = []\n",
    "for blk, g in tqdm(grouped, desc=\"Aggregating per block\"):\n",
    "    ev_seq = \" \".join(g[\"EventId\"].tolist())\n",
    "    # block-level label: if any message in block has label==1, then block label=1 (consistent with anomaly_label.csv)\n",
    "    blk_label = int(g[\"Label\"].max()) if \"Label\" in g else 0\n",
    "    # optional: earliest timestamp of block\n",
    "    start_ts = g[\"Timestamp\"].iloc[0] if not g.empty else \"\"\n",
    "    block_rows.append({\"BlockId\": blk, \"EventSequence\": ev_seq, \"Label\": blk_label, \"StartTimestamp\": start_ts})\n",
    "\n",
    "df_blocks = pd.DataFrame(block_rows)\n",
    "blocks_path = os.path.join(output_dir, f\"{base_name}_event_traces.csv\")\n",
    "df_blocks.to_csv(blocks_path, index=False, encoding=\"utf-8\")\n",
    "print(\"✅ Saved block-level event traces:\", blocks_path)\n",
    "print(\"Total unique blocks:\", len(df_blocks))\n",
    "\n",
    "# -------------------------------\n",
    "# Quick checks / prints (top templates + shapes)\n",
    "# -------------------------------\n",
    "print(\"\\nTop 10 templates (by occurrence):\")\n",
    "print(df_temps.head(10).to_string(index=False))\n",
    "print(\"\\nStructured head:\")\n",
    "print(df_struct.head(6).to_string(index=False))\n",
    "print(f\"\\nStructured rows: {len(df_struct):,}, Templates: {len(df_temps):,}, Blocks: {len(df_blocks):,}\")\n",
    "\n",
    "# Done\n",
    "print(\"\\nParsing done — outputs saved. Next: representation (e.g., TF-IDF/Messages Count) and model construction per replication plan.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e033077-7023-4186-875b-496444168e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading event traces from: D:\\data anomaly\\result_new\\HDFS_event_traces.csv\n",
      "✅ Loaded 575062 block-level traces\n",
      "                    BlockId  \\\n",
      "0                       NaN   \n",
      "1  blk_-1000002529962039464   \n",
      "2   blk_-100000266894974466   \n",
      "3  blk_-1000007292892887521   \n",
      "4  blk_-1000014584150379967   \n",
      "\n",
      "                                       EventSequence  Label  \\\n",
      "0                                           1ea271b9      0   \n",
      "1  73d6de92 73d6de92 73d6de92 0303acc9 73d6de92 2...      0   \n",
      "2  0303acc9 76c99562 76c99562 76c99562 76c99562 7...      0   \n",
      "3  73d6de92 73d6de92 0303acc9 73d6de92 73d6de92 2...      0   \n",
      "4  76c99562 0303acc9 76c99562 76c99562 76c99562 7...      0   \n",
      "\n",
      "          StartTimestamp  \n",
      "0              Timestamp  \n",
      "1  \"2008-11-11 09:44:52\"  \n",
      "2  \"2008-11-10 02:01:06\"  \n",
      "3  \"2008-11-11 09:47:15\"  \n",
      "4  \"2008-11-10 01:51:17\"  \n",
      "Detected columns: ['BlockId', 'EventSequence', 'Label', 'StartTimestamp']\n",
      "🔧 Building TF-IDF representation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing: 100%|██████████████████████████████████████████████████████████| 575062/575062 [00:17<00:00, 33506.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TF-IDF feature shape: (575062, 11)\n",
      "✅ Number of unique EventIds (vocabulary size): 11\n",
      "✅ Saved TF-IDF matrix → D:\\data anomaly\\result_new\\HDFS_tfidf_features.npz\n",
      "✅ Saved labels → D:\\data anomaly\\result_new\\HDFS_tfidf_labels.csv\n",
      "\n",
      "🧩 Sanity check:\n",
      "Feature matrix shape: (575062, 11)\n",
      "Label distribution:\n",
      "Label\n",
      "0    558224\n",
      "1     16838\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "# ==============================================================\n",
    "# ⚙️ Step 1: 基础路径配置\n",
    "# ==============================================================\n",
    "input_file = r\"D:\\data anomaly\\result_new\\HDFS_event_traces.csv\"\n",
    "output_dir = r\"D:\\data anomaly\\result_new\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ==============================================================\n",
    "# 🧩 Step 2: 读取聚合好的 Block-level Event Trace\n",
    "# ==============================================================\n",
    "print(f\"📂 Loading event traces from: {input_file}\")\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} block-level traces\")\n",
    "print(df.head())\n",
    "\n",
    "# 确认字段名（一般为 BlockId, Label, EventSequence）\n",
    "cols = df.columns\n",
    "print(\"Detected columns:\", cols.tolist())\n",
    "\n",
    "# 对 EventSequence 做预处理（有时是空格分隔、有时是逗号分隔）\n",
    "def normalize_sequence(seq):\n",
    "    if isinstance(seq, str):\n",
    "        # 移除多余字符、统一空格\n",
    "        seq = seq.replace(',', ' ').replace(';', ' ').replace('[', '').replace(']', '')\n",
    "        seq = ' '.join(seq.split())\n",
    "        return seq\n",
    "    return ''\n",
    "\n",
    "df[\"EventSequence\"] = df[\"EventSequence\"].apply(normalize_sequence)\n",
    "\n",
    "# ==============================================================\n",
    "# 🧮 Step 3: TF-IDF 表征\n",
    "# ==============================================================\n",
    "print(\"🔧 Building TF-IDF representation ...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',         # 每个 EventId 看作一个单词\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "    lowercase=False,         # EventId 区分大小写\n",
    "    min_df=1,                # 保留所有模板\n",
    "    max_df=1.0               # 不丢弃高频事件\n",
    ")\n",
    "\n",
    "# 训练 TF-IDF 并转换\n",
    "tfidf_matrix = vectorizer.fit_transform(tqdm(df[\"EventSequence\"], desc=\"Vectorizing\"))\n",
    "\n",
    "print(f\"✅ TF-IDF feature shape: {tfidf_matrix.shape}\")\n",
    "print(f\"✅ Number of unique EventIds (vocabulary size): {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# ==============================================================\n",
    "# 💾 Step 4: 保存特征矩阵与标签\n",
    "# ==============================================================\n",
    "feature_file = os.path.join(output_dir, \"HDFS_tfidf_features.npz\")\n",
    "label_file = os.path.join(output_dir, \"HDFS_tfidf_labels.csv\")\n",
    "\n",
    "save_npz(feature_file, tfidf_matrix)\n",
    "df[[\"BlockId\", \"Label\"]].to_csv(label_file, index=False)\n",
    "\n",
    "print(f\"✅ Saved TF-IDF matrix → {feature_file}\")\n",
    "print(f\"✅ Saved labels → {label_file}\")\n",
    "\n",
    "# ==============================================================\n",
    "# 🔍 Step 5: 简单检查\n",
    "# ==============================================================\n",
    "print(\"\\n🧩 Sanity check:\")\n",
    "print(\"Feature matrix shape:\", tfidf_matrix.shape)\n",
    "print(\"Label distribution:\")\n",
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8e7c0-c687-4d35-a753-0c4482afff30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-logparser]",
   "language": "python",
   "name": "conda-env-.conda-logparser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
