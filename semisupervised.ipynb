{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eba2e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py (from keras)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy in /home/andy/andyVenv/lib/python3.12/site-packages (from keras) (2.3.3)\n",
      "Collecting rich (from keras)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: packaging in /home/andy/andyVenv/lib/python3.12/site-packages (from keras) (25.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/andy/andyVenv/lib/python3.12/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/andy/andyVenv/lib/python3.12/site-packages (from tensorflow) (70.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/andy/andyVenv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/andy/andyVenv/lib/python3.12/site-packages (from tensorflow) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.0.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/andy/andyVenv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andy/andyVenv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/andy/andyVenv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andy/andyVenv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /home/andy/andyVenv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/andy/andyVenv/lib/python3.12/site-packages (from rich->keras) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/andy/andyVenv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.5)\n",
      "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-2.0.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m408.8/408.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt_einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google_pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.11.3 libclang-18.1.1 markdown-3.9 markdown-it-py-4.0.0 mdurl-0.1.2 ml-dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 protobuf-6.33.0 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357534cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pyarrow\n",
    "import time\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models import auto_encoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e89e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/andy/6309GP\n",
      "Loading: sequence_hdfs_20251024_183457.bin\n",
      "Index(['SessionId', 'EventSequence', 'Label'], dtype='object')\n",
      "(575061, 3)\n",
      "                  SessionId  \\\n",
      "0  blk_-1608999687919862906   \n",
      "1   blk_7503483334202473044   \n",
      "2  blk_-3544583377289625738   \n",
      "3  blk_-9073992586687739851   \n",
      "4   blk_7854771516489510256   \n",
      "\n",
      "                                       EventSequence  Label  \n",
      "0  E5 E22 E5 E5 E11 E11 E9 E9 E11 E9 E26 E26 E26 ...      0  \n",
      "1  E5 E5 E22 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26 ...      0  \n",
      "2  E5 E22 E5 E5 E11 E9 E11 E9 E11 E9 E3 E26 E26 E...      1  \n",
      "3  E5 E22 E5 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26 ...      0  \n",
      "4  E5 E5 E22 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26 ...      0  \n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"./\")\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "# Find all matching files\n",
    "files = sorted(glob.glob(\"sequence_hdfs_*.bin\"), key=os.path.getmtime)\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No sequence_hdfs_*.bin files found!\")\n",
    "# Pick the most recent one\n",
    "latest_file = files[-1]\n",
    "print(f\"Loading: {latest_file}\")\n",
    "# Load as Feather\n",
    "df_sequence = pd.read_feather(latest_file)\n",
    "print(df_sequence.columns)\n",
    "print(df_sequence.shape)\n",
    "print(df_sequence.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f0a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402542, 3) (172519, 3)\n",
      "Label\n",
      "0    0.970719\n",
      "1    0.029281\n",
      "Name: proportion, dtype: float64\n",
      "Label\n",
      "0    0.970722\n",
      "1    0.029278\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_sequence_dataset(df, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a sequence dataset into training and evaluation sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns ['SessionId', 'EventSequence', 'Label'].\n",
    "    test_size : float\n",
    "        Fraction of the dataset to reserve for evaluation (default 0.3).\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sequence_train : pd.DataFrame\n",
    "    sequence_eval : pd.DataFrame\n",
    "    \"\"\"\n",
    "    sequence_train, sequence_eval = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=df[\"Label\"],  # preserve label distribution\n",
    "        shuffle=True\n",
    "    )\n",
    "    return sequence_train.reset_index(drop=True), sequence_eval.reset_index(drop=True)\n",
    "\n",
    "sequence_train, sequence_eval = split_sequence_dataset(df_sequence)\n",
    "print(sequence_train.shape, sequence_eval.shape)\n",
    "print(sequence_train[\"Label\"].value_counts(normalize=True))\n",
    "print(sequence_eval[\"Label\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86dd9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390755, 3)\n",
      "Label\n",
      "0    390755\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_normal_subset(df_train, label_col=\"Label\"):\n",
    "    return df_train[df_train[label_col] == 0].reset_index(drop=True)\n",
    "\n",
    "sequence_train_normal = get_normal_subset(sequence_train)\n",
    "print(sequence_train_normal.shape)\n",
    "print(sequence_train_normal[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbd729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/andyVenv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train normal: (390755, 29) Eval: (172519, 29)\n"
     ]
    }
   ],
   "source": [
    "def identity_tokenizer(s: str):\n",
    "    # For pre-tokenized input (space-separated)\n",
    "    return s.split()\n",
    "\n",
    "tfidf_id = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False, preprocessor=None)\n",
    "tfidf_matrix_id = tfidf_id.fit_transform(df_sequence[\"EventSequence\"].astype(str))\n",
    "\n",
    "feature_names_id = np.array(tfidf_id.get_feature_names_out())\n",
    "X = tfidf_id.fit_transform(df_sequence[\"EventSequence\"])\n",
    "y = df_sequence[\"Label\"].values\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "mask_normal = (y_train == 0)\n",
    "X_train_normal = X_train[mask_normal]\n",
    "print(\"Train normal:\", X_train_normal.shape, \"Eval:\", X_eval.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2e358",
   "metadata": {},
   "source": [
    "threat to validity: the use of autoencoder expects a matrix with the same size. This is a poor combination when applied with MCV and TF-IDF: \n",
    "\n",
    "IF there are novel templates in eval set in comparison to training set, the autoencoder breaks. \n",
    "\n",
    "In post, it would be better to use an embedding such as word to vect where the matrix is of a fixed dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9b613d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_autoencoder_results(result_df):\n",
    "    \"\"\"\n",
    "    Compute classification metrics from AutoEncoder results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_df : pd.DataFrame\n",
    "        Must contain columns: ['true_label', 'pred_label', 'reconstruction_error']\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dictionary of precision, recall, f1, and auc scores.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = result_df[\"true_label\"].astype(int)\n",
    "    y_pred = result_df[\"pred_label\"].astype(int)\n",
    "    y_score = result_df[\"reconstruction_error\"].values\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    # ROC AUC can be computed on the reconstruction error scores\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    metrics = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "\n",
    "    print(\"\\nğŸ“Š Evaluation metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:10s}: {v:.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25a9996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "\n",
    "def train_and_evaluate_autoencoder(X_train, X_eval, y_train, y_eval,\n",
    "                                   contamination=0.001, hidden_neurons=[64, 32],\n",
    "                                   epochs=20, batch_size=32, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train a PyOD AutoEncoder on X_train (normal samples) and evaluate on X_eval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array-like or sparse matrix\n",
    "        Training feature matrix (only normal samples).\n",
    "    X_eval : array-like or sparse matrix\n",
    "        Evaluation feature matrix.\n",
    "    y_train, y_eval : array-like\n",
    "        Labels for training and evaluation sets.\n",
    "    contamination : float\n",
    "        Expected proportion of anomalies in the data.\n",
    "    hidden_neurons : list\n",
    "        Hidden layer sizes for encoder/decoder symmetry.\n",
    "    epochs : int\n",
    "        Number of training epochs.\n",
    "    batch_size : int\n",
    "        Training batch size.\n",
    "    lr : float\n",
    "        Learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result_df : pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "        ['true_label', 'pred_label', 'reconstruction_error']\n",
    "    model : AutoEncoder\n",
    "        Trained PyOD autoencoder model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training AutoEncoder on {X_train.shape[0]} samples, \"\n",
    "          f\"evaluating on {X_eval.shape[0]} samples...\")\n",
    "\n",
    "    # Initialize and fit AutoEncoder\n",
    "    model = AutoEncoder(hidden_neuron_list=hidden_neurons,\n",
    "                        contamination=contamination,\n",
    "                        epoch_num=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        lr=lr,\n",
    "                        verbose=1)\n",
    "\n",
    "    model.fit(X_train.toarray())\n",
    "\n",
    "    # Predict on evaluation set\n",
    "    y_pred = model.predict(X_eval.toarray())                     # binary labels (0=inlier, 1=outlier)\n",
    "    scores = model.decision_function(X_eval.toarray())           # reconstruction errors\n",
    "\n",
    "    # Build output dataframe\n",
    "    result_df = pd.DataFrame({\n",
    "        \"true_label\": y_eval,\n",
    "        \"pred_label\": y_pred,\n",
    "        \"reconstruction_error\": scores\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Training complete.\")\n",
    "    print(result_df[\"pred_label\"].value_counts())\n",
    "    print(\"Mean reconstruction error:\", np.mean(scores))\n",
    "\n",
    "    return result_df, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a94c8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_reconstruction_error_by_label(result_df):\n",
    "    \"\"\"\n",
    "    Compute average and standard deviation of reconstruction errors\n",
    "    for normal (label=0) and anomaly (label=1) samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_df : pd.DataFrame\n",
    "        DataFrame with at least columns ['true_label', 'reconstruction_error'].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summary_df : pd.DataFrame\n",
    "        Summary of mean and std reconstruction error by label.\n",
    "    \"\"\"\n",
    "\n",
    "    if not {\"true_label\", \"reconstruction_error\"}.issubset(result_df.columns):\n",
    "        raise ValueError(\"result_df must contain 'true_label' and 'reconstruction_error' columns\")\n",
    "\n",
    "    summary = (\n",
    "        result_df.groupby(\"true_label\")[\"reconstruction_error\"]\n",
    "        .agg([\"mean\", \"std\", \"min\", \"max\", \"count\"])\n",
    "        .rename(index={0: \"Normal (0)\", 1: \"Anomaly (1)\"})\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ“Š Reconstruction Error Summary by Label:\")\n",
    "    print(summary)\n",
    "\n",
    "    diff_ratio = summary.loc[\"Anomaly (1)\", \"mean\"] / summary.loc[\"Normal (0)\", \"mean\"]\n",
    "    print(f\"\\nMean error ratio (Anomaly / Normal): {diff_ratio:.2f}\")\n",
    "\n",
    "    if diff_ratio < 1.2:\n",
    "        print(\"âš ï¸  Weak separation â€” model may be underfitting or over-regularized.\")\n",
    "    elif diff_ratio > 2.0:\n",
    "        print(\"âœ…  Strong separation â€” anomalies reconstruct significantly worse.\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸  Moderate separation â€” may improve with tuning (epochs, hidden layers, or dropout).\")\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b20959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:29<00:00, 12.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    151367\n",
      "1     21152\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           1             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           1              1.399192\n",
      "9           0           1              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.2388\n",
      "recall    : 1.0000\n",
      "f1_score  : 0.3855\n",
      "auc       : 0.9989\n",
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:27<00:00, 12.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    165871\n",
      "1      6648\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           1             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           1              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.7598\n",
      "recall    : 1.0000\n",
      "f1_score  : 0.8635\n",
      "auc       : 0.9989\n",
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:34<00:00, 13.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    166721\n",
      "1      5798\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           1             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           0              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.8712\n",
      "recall    : 1.0000\n",
      "f1_score  : 0.9311\n",
      "auc       : 0.9989\n",
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:38<00:00, 13.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    169034\n",
      "1      3485\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           0             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           0              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.9529\n",
      "recall    : 0.6575\n",
      "f1_score  : 0.7781\n",
      "auc       : 0.9989\n"
     ]
    }
   ],
   "source": [
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.1, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n",
    "\n",
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.01, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n",
    "\n",
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.005, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n",
    "\n",
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.001, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6e029b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:21<00:00, 12.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    165871\n",
      "1      6648\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           1             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           1              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.7598\n",
      "recall    : 1.0000\n",
      "f1_score  : 0.8635\n",
      "auc       : 0.9989\n",
      "\n",
      "ğŸ“Š Reconstruction Error Summary by Label:\n",
      "                     mean           std       min           max   count\n",
      "true_label                                                             \n",
      "Normal (0)   4.107166e-01  2.283976e+00  0.054559  1.272052e+02  167468\n",
      "Anomaly (1)  2.071729e+07  3.892227e+07  3.614228  9.946272e+07    5051\n",
      "\n",
      "Mean error ratio (Anomaly / Normal): 50441812.00\n",
      "âœ…  Strong separation â€” anomalies reconstruct significantly worse.\n",
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:19<00:00, 12.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    166721\n",
      "1      5798\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           1             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           0              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.8712\n",
      "recall    : 1.0000\n",
      "f1_score  : 0.9311\n",
      "auc       : 0.9989\n",
      "\n",
      "ğŸ“Š Reconstruction Error Summary by Label:\n",
      "                     mean           std       min           max   count\n",
      "true_label                                                             \n",
      "Normal (0)   4.107166e-01  2.283976e+00  0.054559  1.272052e+02  167468\n",
      "Anomaly (1)  2.071729e+07  3.892227e+07  3.614228  9.946272e+07    5051\n",
      "\n",
      "Mean error ratio (Anomaly / Normal): 50441812.00\n",
      "âœ…  Strong separation â€” anomalies reconstruct significantly worse.\n",
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [03:18<03:18, 13.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m metrics = evaluate_autoencoder_results(result_df)\n\u001b[32m     17\u001b[39m analyze_reconstruction_error_by_label(result_df)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m result_df, ae_model = \u001b[43mtrain_and_evaluate_autoencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_normal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontamination\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_neurons\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(result_df.head(\u001b[32m10\u001b[39m))\n\u001b[32m     25\u001b[39m metrics = evaluate_autoencoder_results(result_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain_and_evaluate_autoencoder\u001b[39m\u001b[34m(X_train, X_eval, y_train, y_eval, contamination, hidden_neurons, epochs, batch_size, lr)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Initialize and fit AutoEncoder\u001b[39;00m\n\u001b[32m     43\u001b[39m model = AutoEncoder(hidden_neuron_list=hidden_neurons,\n\u001b[32m     44\u001b[39m                     contamination=contamination,\n\u001b[32m     45\u001b[39m                     epoch_num=epochs,\n\u001b[32m     46\u001b[39m                     batch_size=batch_size,\n\u001b[32m     47\u001b[39m                     lr=lr,\n\u001b[32m     48\u001b[39m                     verbose=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Predict on evaluation set\u001b[39;00m\n\u001b[32m     53\u001b[39m y_pred = model.predict(X_eval.toarray())                     \u001b[38;5;66;03m# binary labels (0=inlier, 1=outlier)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/pyod/models/base_dl.py:200\u001b[39m, in \u001b[36mBaseDeepLearningDetector.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    195\u001b[39m train_loader = torch.utils.data.DataLoader(\n\u001b[32m    196\u001b[39m     dataset=train_set, batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m    197\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28mself\u001b[39m.decision_scores_ = \u001b[38;5;28mself\u001b[39m.decision_function(X)\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m._process_decision_scores()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/pyod/models/base_dl.py:234\u001b[39m, in \u001b[36mBaseDeepLearningDetector.train\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m    232\u001b[39m start_time = time.time()\n\u001b[32m    233\u001b[39m overall_loss = []\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverall_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.01, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n",
    "analyze_reconstruction_error_by_label(result_df)\n",
    "\n",
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.005, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n",
    "analyze_reconstruction_error_by_label(result_df)\n",
    "\n",
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.001, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n",
    "analyze_reconstruction_error_by_label(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c25d804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:30<00:00, 13.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    169034\n",
      "1      3485\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           0             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           0              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.9529\n",
      "recall    : 0.6575\n",
      "f1_score  : 0.7781\n",
      "auc       : 0.9989\n",
      "\n",
      "ğŸ“Š Reconstruction Error Summary by Label:\n",
      "                     mean           std       min           max   count\n",
      "true_label                                                             \n",
      "Normal (0)   4.107166e-01  2.283976e+00  0.054559  1.272052e+02  167468\n",
      "Anomaly (1)  2.071729e+07  3.892227e+07  3.614228  9.946272e+07    5051\n",
      "\n",
      "Mean error ratio (Anomaly / Normal): 50441812.00\n",
      "âœ…  Strong separation â€” anomalies reconstruct significantly worse.\n",
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:40<00:00, 13.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    168392\n",
      "1      4127\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           1             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           0              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.8885\n",
      "recall    : 0.7260\n",
      "f1_score  : 0.7991\n",
      "auc       : 0.9989\n",
      "\n",
      "ğŸ“Š Reconstruction Error Summary by Label:\n",
      "                     mean           std       min           max   count\n",
      "true_label                                                             \n",
      "Normal (0)   4.107166e-01  2.283976e+00  0.054559  1.272052e+02  167468\n",
      "Anomaly (1)  2.071729e+07  3.892227e+07  3.614228  9.946272e+07    5051\n",
      "\n",
      "Mean error ratio (Anomaly / Normal): 50441812.00\n",
      "âœ…  Strong separation â€” anomalies reconstruct significantly worse.\n",
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:40<00:00, 13.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    166812\n",
      "1      5707\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           1             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           0              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.8851\n",
      "recall    : 1.0000\n",
      "f1_score  : 0.9390\n",
      "auc       : 0.9989\n",
      "\n",
      "ğŸ“Š Reconstruction Error Summary by Label:\n",
      "                     mean           std       min           max   count\n",
      "true_label                                                             \n",
      "Normal (0)   4.107166e-01  2.283976e+00  0.054559  1.272052e+02  167468\n",
      "Anomaly (1)  2.071729e+07  3.892227e+07  3.614228  9.946272e+07    5051\n",
      "\n",
      "Mean error ratio (Anomaly / Normal): 50441812.00\n",
      "âœ…  Strong separation â€” anomalies reconstruct significantly worse.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Normal (0)</th>\n",
       "      <td>4.107166e-01</td>\n",
       "      <td>2.283976e+00</td>\n",
       "      <td>0.054559</td>\n",
       "      <td>1.272052e+02</td>\n",
       "      <td>167468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anomaly (1)</th>\n",
       "      <td>2.071729e+07</td>\n",
       "      <td>3.892227e+07</td>\n",
       "      <td>3.614228</td>\n",
       "      <td>9.946272e+07</td>\n",
       "      <td>5051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean           std       min           max   count\n",
       "true_label                                                             \n",
       "Normal (0)   4.107166e-01  2.283976e+00  0.054559  1.272052e+02  167468\n",
       "Anomaly (1)  2.071729e+07  3.892227e+07  3.614228  9.946272e+07    5051"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.002, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n",
    "analyze_reconstruction_error_by_label(result_df)\n",
    "\n",
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.003, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n",
    "analyze_reconstruction_error_by_label(result_df)\n",
    "\n",
    "result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.004, hidden_neurons=[128, 64], epochs=30\n",
    ")\n",
    "\n",
    "print(result_df.head(10))\n",
    "metrics = evaluate_autoencoder_results(result_df)\n",
    "analyze_reconstruction_error_by_label(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d656025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:54<00:00, 13.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    169034\n",
      "1      3485\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           0             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           0              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.9529\n",
      "recall    : 0.6575\n",
      "f1_score  : 0.7781\n",
      "auc       : 0.9989\n",
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [06:34<00:00, 13.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Training complete.\n",
      "pred_label\n",
      "0    169034\n",
      "1      3485\n",
      "Name: count, dtype: int64\n",
      "Mean reconstruction error: 606559.75\n",
      "   true_label  pred_label  reconstruction_error\n",
      "0           0           0              0.054559\n",
      "1           0           0              0.054559\n",
      "2           0           0              0.054559\n",
      "3           0           0             38.753941\n",
      "4           0           0              0.054559\n",
      "5           0           0              0.380731\n",
      "6           0           0              0.054559\n",
      "7           0           0              0.054559\n",
      "8           0           0              1.399192\n",
      "9           0           0              0.796620\n",
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.9529\n",
      "recall    : 0.6575\n",
      "f1_score  : 0.7781\n",
      "auc       : 0.9989\n",
      "Training AutoEncoder on 390755 samples, evaluating on 172519 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆ         | 3/30 [00:46<07:01, 15.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[32m0\u001b[39m,\u001b[32m20\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     result_df, ae_model = \u001b[43mtrain_and_evaluate_autoencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_normal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontamination\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_neurons\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result_df.head(\u001b[32m10\u001b[39m))\n\u001b[32m      8\u001b[39m     metrics = evaluate_autoencoder_results(result_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain_and_evaluate_autoencoder\u001b[39m\u001b[34m(X_train, X_eval, y_train, y_eval, contamination, hidden_neurons, epochs, batch_size, lr)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Initialize and fit AutoEncoder\u001b[39;00m\n\u001b[32m     43\u001b[39m model = AutoEncoder(hidden_neuron_list=hidden_neurons,\n\u001b[32m     44\u001b[39m                     contamination=contamination,\n\u001b[32m     45\u001b[39m                     epoch_num=epochs,\n\u001b[32m     46\u001b[39m                     batch_size=batch_size,\n\u001b[32m     47\u001b[39m                     lr=lr,\n\u001b[32m     48\u001b[39m                     verbose=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Predict on evaluation set\u001b[39;00m\n\u001b[32m     53\u001b[39m y_pred = model.predict(X_eval.toarray())                     \u001b[38;5;66;03m# binary labels (0=inlier, 1=outlier)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/pyod/models/base_dl.py:200\u001b[39m, in \u001b[36mBaseDeepLearningDetector.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    195\u001b[39m train_loader = torch.utils.data.DataLoader(\n\u001b[32m    196\u001b[39m     dataset=train_set, batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m    197\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28mself\u001b[39m.decision_scores_ = \u001b[38;5;28mself\u001b[39m.decision_function(X)\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m._process_decision_scores()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/pyod/models/base_dl.py:235\u001b[39m, in \u001b[36mBaseDeepLearningDetector.train\u001b[39m\u001b[34m(self, train_loader)\u001b[39m\n\u001b[32m    233\u001b[39m overall_loss = []\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     overall_loss.append(loss)\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# loss could be a tuple or a single value\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/pyod/models/auto_encoder.py:159\u001b[39m, in \u001b[36mAutoEncoder.training_forward\u001b[39m\u001b[34m(self, batch_data)\u001b[39m\n\u001b[32m    157\u001b[39m x = batch_data\n\u001b[32m    158\u001b[39m x = x.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m x_recon = \u001b[38;5;28mself\u001b[39m.model(x)\n\u001b[32m    161\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(x_recon, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/torch/_compile.py:53\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1046\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/torch/optim/optimizer.py:1035\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m         p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad.grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in range (0,20):\n",
    "    result_df, ae_model = train_and_evaluate_autoencoder(\n",
    "    X_train_normal, X_eval, y_train=None, y_eval=y_eval,\n",
    "    contamination=0.001 * (i+1), hidden_neurons=[128, 64], epochs=30\n",
    "    )\n",
    "\n",
    "    print(result_df.head(10))\n",
    "    metrics = evaluate_autoencoder_results(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c5ff352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluation metrics:\n",
      "precision : 0.9529\n",
      "recall    : 0.6575\n",
      "f1_score  : 0.7781\n",
      "auc       : 0.9989\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_autoencoder_results(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d16f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OCSVM(kernel='rbf', gamma='auto', contamination=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 19.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample anomaly predictions: [0 0 0 0 0 0 0 0 0 0]\n",
      "Sample reconstruction errors: [1.7826098 1.1180751 1.8728502 1.4407952 1.6203517 1.7224503 1.6781187\n",
      " 1.2661917 1.4927775 2.1730125]\n"
     ]
    }
   ],
   "source": [
    "#pyod example\n",
    "from pyod.models import auto_encoder\n",
    "from pyod.utils.data import generate_data\n",
    "auto_encoder.AutoEncoder()\n",
    "\n",
    "# Generate synthetic data\n",
    "X_train, X_test, y_train, y_test = generate_data(n_train=500, n_test=100, n_features=20)\n",
    "\n",
    "# Initialize AutoEncoder\n",
    "clf = auto_encoder.AutoEncoder(\n",
    "    hidden_neuron_list=[64, 32],\n",
    "    epoch_num=10,\n",
    "    batch_size=32,\n",
    "    device='cuda',   # or 'cpu' if no GPU\n",
    "    contamination=0.1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Predict anomalies\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print sample results\n",
    "print(\"Sample anomaly predictions:\", y_pred[:10])\n",
    "print(\"Sample reconstruction errors:\", clf.decision_scores_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0284748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "def train_and_evaluate_ocsvm(X_train, X_eval, y_train, y_eval,\n",
    "                             kernel='rbf', nu=0.05, gamma='auto', contamination=0.05):\n",
    "    \"\"\"\n",
    "    Train a PyOD One-Class SVM (OCSVM) on X_train (normal samples) and evaluate on X_eval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array-like or sparse matrix\n",
    "        Training feature matrix (only normal samples).\n",
    "    X_eval : array-like or sparse matrix\n",
    "        Evaluation feature matrix.\n",
    "    y_train, y_eval : array-like\n",
    "        Labels for training and evaluation sets.\n",
    "    kernel : str\n",
    "        Kernel type ('linear', 'poly', 'rbf', 'sigmoid').\n",
    "    nu : float\n",
    "        Expected fraction of outliers in training data (upper bound).\n",
    "    gamma : str or float\n",
    "        Kernel coefficient ('scale', 'auto', or numeric value).\n",
    "    contamination : float\n",
    "        Expected fraction of anomalies in the data (used for thresholding).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result_df : pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "        ['true_label', 'pred_label', 'anomaly_score']\n",
    "    model : OCSVM\n",
    "        Trained PyOD OCSVM model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training OCSVM on {X_train.shape[0]} normal samples, \"\n",
    "          f\"evaluating on {X_eval.shape[0]} samples...\")\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = OCSVM(kernel=kernel, nu=nu, gamma=gamma, contamination=contamination)\n",
    "    model.fit(X_train.toarray())\n",
    "\n",
    "    # Predict on evaluation set\n",
    "    y_pred = model.predict(X_eval.toarray())              # 0=inlier, 1=outlier\n",
    "    scores = model.decision_function(X_eval.toarray())    # anomaly scores (higher = more normal)\n",
    "\n",
    "    # Build result DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        \"true_label\": y_eval,\n",
    "        \"pred_label\": y_pred,\n",
    "        \"anomaly_score\": scores\n",
    "    })\n",
    "\n",
    "    # Display summary\n",
    "    print(\"\\nâœ… Training complete.\")\n",
    "    print(\"Predicted label counts:\")\n",
    "    print(result_df[\"pred_label\"].value_counts())\n",
    "    print(\"Mean anomaly score:\", np.mean(scores))\n",
    "\n",
    "    return result_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f9e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training OCSVM on 390755 normal samples, evaluating on 172519 samples...\n"
     ]
    }
   ],
   "source": [
    "# def identity_tokenizer(s: str):\n",
    "#     # For pre-tokenized input (space-separated)\n",
    "#     return s.split()\n",
    "\n",
    "# tfidf_id = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False, preprocessor=None)\n",
    "# tfidf_matrix_id = tfidf_id.fit_transform(df_sequence[\"EventSequence\"].astype(str))\n",
    "\n",
    "# feature_names_id = np.array(tfidf_id.get_feature_names_out())\n",
    "# X = tfidf_id.fit_transform(df_sequence[\"EventSequence\"])\n",
    "# y = df_sequence[\"Label\"].values\n",
    "\n",
    "# X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# mask_normal = (y_train == 0)\n",
    "# X_train_normal = X_train[mask_normal]\n",
    "# print(\"Train normal:\", X_train_normal.shape, \"Eval:\", X_eval.shape)\n",
    "\n",
    "# Train and evaluate\n",
    "result_df, ocsvm_model = train_and_evaluate_ocsvm(\n",
    "    X_train=X_train_normal,\n",
    "    X_eval=X_eval,\n",
    "    y_train=y_train,\n",
    "    y_eval=y_eval,\n",
    "    kernel='rbf',\n",
    "    nu=0.05,\n",
    "    gamma='scale',\n",
    "    contamination=0.05\n",
    ")\n",
    "\n",
    "# Optional: evaluate performance metrics\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(classification_report(result_df[\"true_label\"], result_df[\"pred_label\"]))\n",
    "print(\"ROC AUC:\", roc_auc_score(result_df[\"true_label\"], result_df[\"anomaly_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f00a79",
   "metadata": {},
   "source": [
    "Takes too long. may i try smth else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f473a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyod.models.iforest import IForest\n",
    "\n",
    "def train_and_evaluate_iforest(X_train, X_eval, y_train, y_eval,\n",
    "                               contamination=0.05, n_estimators=100,\n",
    "                               max_samples='auto', n_jobs=-1, random_state=42):\n",
    "    print(f\"training IForest contamination={contamination}, n_estimators={n_estimators}, max_samples={max_samples}, n_jobs={n_jobs}, random_state={random_state}\")\n",
    "    \"\"\"\n",
    "    Train a PyOD Isolation Forest on X_train (normal samples) and evaluate on X_eval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array-like or sparse matrix\n",
    "        Training feature matrix (only normal samples).\n",
    "    X_eval : array-like or sparse matrix\n",
    "        Evaluation feature matrix.\n",
    "    y_train, y_eval : array-like\n",
    "        Labels for training and evaluation sets.\n",
    "    contamination : float\n",
    "        Expected proportion of anomalies in the data.\n",
    "    n_estimators : int\n",
    "        Number of base estimators (trees).\n",
    "    max_samples : int or 'auto'\n",
    "        Number of samples to draw to train each base estimator.\n",
    "    n_jobs : int\n",
    "        Number of parallel threads to use (-1 = all cores).\n",
    "    random_state : int\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result_df : pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "        ['true_label', 'pred_label', 'anomaly_score']\n",
    "    model : IForest\n",
    "        Trained PyOD Isolation Forest model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training IsolationForest on {X_train.shape[0]} normal samples, \"\n",
    "          f\"evaluating on {X_eval.shape[0]} samples...\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = IForest(contamination=contamination,\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_samples=max_samples,\n",
    "                    n_jobs=n_jobs,\n",
    "                    random_state=random_state)\n",
    "\n",
    "    # Fit model on training data\n",
    "    model.fit(X_train.toarray())\n",
    "\n",
    "    # Predict on evaluation set\n",
    "    y_pred = model.predict(X_eval.toarray())               # 0=inlier, 1=outlier\n",
    "    scores = model.decision_function(X_eval.toarray())     # anomaly scores (higher = more normal)\n",
    "\n",
    "    # Build output dataframe\n",
    "    result_df = pd.DataFrame({\n",
    "        \"true_label\": y_eval,\n",
    "        \"pred_label\": y_pred,\n",
    "        \"anomaly_score\": scores\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Training complete.\")\n",
    "    print(\"Predicted label counts:\")\n",
    "    print(result_df[\"pred_label\"].value_counts())\n",
    "    print(\"Mean anomaly score:\", np.mean(scores))\n",
    "\n",
    "    return result_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "826c6816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IsolationForest on 390755 normal samples, evaluating on 172519 samples...\n",
      "\n",
      "âœ… Training complete.\n",
      "Predicted label counts:\n",
      "pred_label\n",
      "0    161678\n",
      "1     10841\n",
      "Name: count, dtype: int64\n",
      "Mean anomaly score: -0.13385453171097553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97    167468\n",
      "           1       0.33      0.70      0.45      5051\n",
      "\n",
      "    accuracy                           0.95    172519\n",
      "   macro avg       0.66      0.83      0.71    172519\n",
      "weighted avg       0.97      0.95      0.96    172519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df, iforest_model = train_and_evaluate_iforest(\n",
    "    X_train=X_train_normal,\n",
    "    X_eval=X_eval,\n",
    "    y_train=y_train,\n",
    "    y_eval=y_eval,\n",
    "    contamination=0.05,\n",
    "    n_estimators=10000,\n",
    "    n_jobs=-1  # âœ… use all cores\n",
    ")\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(classification_report(result_df[\"true_label\"], result_df[\"pred_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33b8bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyod.models.iforest import IForest\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "def tune_iforest_hyperparams(X_train, X_eval, y_train, y_eval,\n",
    "                             param_grid=None,\n",
    "                             score_metric='f1_anomaly',\n",
    "                             n_jobs=-1, random_state=42):\n",
    "    \"\"\"\n",
    "    Systematically explore IForest hyperparameters via combinatorial search,\n",
    "    using only the anomaly (label==1) class metrics for evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array-like or sparse matrix\n",
    "        Training feature matrix (normal samples only).\n",
    "    X_eval : array-like or sparse matrix\n",
    "        Evaluation feature matrix (mixed normal + anomalies).\n",
    "    y_train, y_eval : array-like\n",
    "        True labels for training and evaluation sets.\n",
    "    param_grid : dict or None\n",
    "        Dictionary of parameter lists to try (like sklearn's ParameterGrid).\n",
    "        If None, defaults to a reasonable small search space.\n",
    "    score_metric : {'f1_anomaly', 'recall_anomaly', 'precision_anomaly', 'roc_auc'}\n",
    "        Metric used to rank models. 'roc_auc' still uses all labels.\n",
    "    n_jobs : int\n",
    "        Number of parallel threads for model training (-1 = all cores).\n",
    "    random_state : int\n",
    "        Random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pd.DataFrame\n",
    "        Each row corresponds to one parameter set and its evaluation metrics.\n",
    "    best_model : IForest\n",
    "        Trained model with best parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_estimators': [2000],\n",
    "            'max_samples': [0.5],\n",
    "            'max_features': [0.5, 0.8, 1.0],\n",
    "            'contamination': [0.01],\n",
    "            'bootstrap': [False, True],\n",
    "        }\n",
    "\n",
    "    print(f\"ğŸ” Starting grid search over {len(ParameterGrid(param_grid))} combinations...\")\n",
    "\n",
    "    X_train_dense = X_train.toarray()\n",
    "    X_eval_dense = X_eval.toarray()\n",
    "\n",
    "    records = []\n",
    "    best_score = -np.inf\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"\\nâ–¶ Training with params: {params}\")\n",
    "\n",
    "        model = IForest(\n",
    "            **params,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        model.fit(X_train_dense)\n",
    "        scores = model.decision_function(X_eval_dense)\n",
    "        preds = model.predict(X_eval_dense)\n",
    "\n",
    "        # Compute ROC-AUC across all samples\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_eval, scores)\n",
    "        except ValueError:\n",
    "            roc_auc = np.nan\n",
    "\n",
    "        # Compute precision, recall, f1 specifically for anomaly class\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_eval, preds, average=None, labels=[1]\n",
    "        )\n",
    "        precision_anom, recall_anom, f1_anom = precision[0], recall[0], f1[0]\n",
    "\n",
    "        record = {\n",
    "            **params,\n",
    "            'roc_auc': roc_auc,\n",
    "            'precision_anomaly': precision_anom,\n",
    "            'recall_anomaly': recall_anom,\n",
    "            'f1_anomaly': f1_anom\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "        # Choose which metric to optimize\n",
    "        metric_value = record.get(score_metric, np.nan)\n",
    "        if np.isnan(metric_value):\n",
    "            metric_value = -np.inf\n",
    "        if metric_value > best_score:\n",
    "            best_score = metric_value\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "\n",
    "        print(f\"â†’ F1(anomaly): {f1_anom:.4f}, Recall(anomaly): {recall_anom:.4f}, \"\n",
    "              f\"Precision(anomaly): {precision_anom:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    results_df = pd.DataFrame(records).sort_values(by=score_metric, ascending=False)\n",
    "    print(\"\\nâœ… Best parameters found:\")\n",
    "    print(best_params)\n",
    "    print(f\"{score_metric.upper()}: {best_score:.4f}\")\n",
    "\n",
    "    return results_df, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cb0cde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Starting grid search over 6 combinations...\n",
      "\n",
      "â–¶ Training with params: {'bootstrap': False, 'contamination': 0.01, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 2000}\n",
      "â†’ F1(anomaly): 0.8675, Recall(anomaly): 1.0000, Precision(anomaly): 0.7660, ROC AUC: 0.9982\n",
      "\n",
      "â–¶ Training with params: {'bootstrap': False, 'contamination': 0.01, 'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 2000}\n",
      "â†’ F1(anomaly): 0.8675, Recall(anomaly): 1.0000, Precision(anomaly): 0.7660, ROC AUC: 0.9984\n",
      "\n",
      "â–¶ Training with params: {'bootstrap': False, 'contamination': 0.01, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 2000}\n",
      "â†’ F1(anomaly): 0.8675, Recall(anomaly): 1.0000, Precision(anomaly): 0.7660, ROC AUC: 0.9984\n",
      "\n",
      "â–¶ Training with params: {'bootstrap': True, 'contamination': 0.01, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 2000}\n",
      "â†’ F1(anomaly): 0.8649, Recall(anomaly): 1.0000, Precision(anomaly): 0.7620, ROC AUC: 0.9981\n",
      "\n",
      "â–¶ Training with params: {'bootstrap': True, 'contamination': 0.01, 'max_features': 0.8, 'max_samples': 0.5, 'n_estimators': 2000}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m mask_normal = (y_train == \u001b[32m0\u001b[39m)\n\u001b[32m      3\u001b[39m X_train_normal = X_train[mask_normal]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results_df, best_model = \u001b[43mtune_iforest_hyperparams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train_normal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mroc_auc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# View ranked results\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(results_df.head(\u001b[32m5\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtune_iforest_hyperparams\u001b[39m\u001b[34m(X_train, X_eval, y_train, y_eval, param_grid, score_metric, n_jobs, random_state)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâ–¶ Training with params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m model = IForest(\n\u001b[32m     67\u001b[39m     **params,\n\u001b[32m     68\u001b[39m     n_jobs=n_jobs,\n\u001b[32m     69\u001b[39m     random_state=random_state\n\u001b[32m     70\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_dense\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m scores = model.decision_function(X_eval_dense)\n\u001b[32m     74\u001b[39m preds = model.predict(X_eval_dense)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/pyod/models/iforest.py:217\u001b[39m, in \u001b[36mIForest.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# In sklearn 0.20+ new behaviour is added (arg behaviour={'new','old'})\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# to IsolationForest that shifts the location of the anomaly scores\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# noinspection PyProtectedMember\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;28mself\u001b[39m.detector_ = IsolationForest(n_estimators=\u001b[38;5;28mself\u001b[39m.n_estimators,\n\u001b[32m    209\u001b[39m                                  max_samples=\u001b[38;5;28mself\u001b[39m.max_samples,\n\u001b[32m    210\u001b[39m                                  contamination=\u001b[38;5;28mself\u001b[39m.contamination,\n\u001b[32m   (...)\u001b[39m\u001b[32m    214\u001b[39m                                  random_state=\u001b[38;5;28mself\u001b[39m.random_state,\n\u001b[32m    215\u001b[39m                                  verbose=\u001b[38;5;28mself\u001b[39m.verbose)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetector_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# invert decision_scores_. Outliers comes with higher outlier scores.\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.decision_scores_ = invert_order(\n\u001b[32m    221\u001b[39m     \u001b[38;5;28mself\u001b[39m.detector_.decision_function(X))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/sklearn/ensemble/_iforest.py:381\u001b[39m, in \u001b[36mIsolationForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[32m    380\u001b[39m     X = X.tocsr()\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28mself\u001b[39m.offset_ = np.percentile(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, \u001b[32m100.0\u001b[39m * \u001b[38;5;28mself\u001b[39m.contamination)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/sklearn/ensemble/_iforest.py:540\u001b[39m, in \u001b[36mIsolationForest._score_samples\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    537\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Take the opposite of the scores as bigger is better (here less abnormal)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_chunked_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/sklearn/ensemble/_iforest.py:570\u001b[39m, in \u001b[36mIsolationForest._compute_chunked_score_samples\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    566\u001b[39m scores = np.zeros(n_samples, order=\u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sl \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[32m    569\u001b[39m     \u001b[38;5;66;03m# compute score on the slices of test samples:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     scores[sl] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_score_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43msl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/sklearn/ensemble/_iforest.py:606\u001b[39m, in \u001b[36mIsolationForest._compute_score_samples\u001b[39m\u001b[34m(self, X, subsample_features)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# Note: we use default n_jobs value, i.e. sequential computation, which\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# we expect to be more performant that parallelizing for small number\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# of samples, e.g. < 1k samples. Default n_jobs value can be overridden\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# https://github.com/scikit-learn/scikit-learn/pull/28622 for more\u001b[39;00m\n\u001b[32m    604\u001b[39m \u001b[38;5;66;03m# details.\u001b[39;00m\n\u001b[32m    605\u001b[39m lock = threading.Lock()\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequire\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msharedmem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_compute_tree_depths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubsample_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decision_path_lengths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtree_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_average_path_length_per_tree\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtree_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimators_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimators_features_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m denominator = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_) * average_path_length_max_samples\n\u001b[32m    625\u001b[39m scores = \u001b[32m2\u001b[39m ** (\n\u001b[32m    626\u001b[39m     \u001b[38;5;66;03m# For a single training sample, denominator and depth are 0.\u001b[39;00m\n\u001b[32m    627\u001b[39m     \u001b[38;5;66;03m# Therefore, we set the score manually to 1.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    630\u001b[39m     )\n\u001b[32m    631\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/sklearn/ensemble/_iforest.py:44\u001b[39m, in \u001b[36m_parallel_compute_tree_depths\u001b[39m\u001b[34m(tree, X, features, tree_decision_path_lengths, tree_avg_path_lengths, depths, lock)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     42\u001b[39m     X_subset = X[:, features]\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m leaves_index = \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[32m     47\u001b[39m     depths += (\n\u001b[32m     48\u001b[39m         tree_decision_path_lengths[leaves_index]\n\u001b[32m     49\u001b[39m         + tree_avg_path_lengths[leaves_index]\n\u001b[32m     50\u001b[39m         - \u001b[32m1.0\u001b[39m\n\u001b[32m     51\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/andyVenv/lib/python3.12/site-packages/sklearn/tree/_classes.py:557\u001b[39m, in \u001b[36mBaseDecisionTree.apply\u001b[39m\u001b[34m(self, X, check_input)\u001b[39m\n\u001b[32m    554\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    555\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m proba[:, :, \u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    558\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the index of the leaf that each sample is predicted as.\u001b[39;00m\n\u001b[32m    559\u001b[39m \n\u001b[32m    560\u001b[39m \u001b[33;03m    .. versionadded:: 0.17\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m \u001b[33;03m        numbering.\u001b[39;00m\n\u001b[32m    580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    581\u001b[39m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train only on normal samples\n",
    "mask_normal = (y_train == 0)\n",
    "X_train_normal = X_train[mask_normal]\n",
    "\n",
    "results_df, best_model = tune_iforest_hyperparams(\n",
    "    X_train=X_train_normal,\n",
    "    X_eval=X_eval,\n",
    "    y_train=y_train,\n",
    "    y_eval=y_eval,\n",
    "    score_metric='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# View ranked results\n",
    "print(results_df.head(5))\n",
    "\n",
    "# Use the best model to predict again if needed\n",
    "y_pred = best_model.predict(X_eval.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e46c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyod.models.ecod import ECOD\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def train_and_evaluate_ecod(X_train, X_eval, y_train, y_eval,\n",
    "                            contamination=0.05):\n",
    "    \"\"\"\n",
    "    Train and evaluate the ECOD anomaly detector (Empirical CDF-based Outlier Detection).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array-like or sparse matrix\n",
    "        Training feature matrix (normal samples only).\n",
    "    X_eval : array-like or sparse matrix\n",
    "        Evaluation feature matrix (mixed normal + anomalies).\n",
    "    y_train, y_eval : array-like\n",
    "        Labels for training and evaluation sets.\n",
    "    contamination : float\n",
    "        Expected fraction of anomalies in the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result_df : pd.DataFrame\n",
    "        Columns: ['true_label', 'pred_label', 'anomaly_score']\n",
    "    model : ECOD\n",
    "        Trained ECOD model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Training ECOD on {X_train.shape[0]} normal samples, \"\n",
    "          f\"evaluating on {X_eval.shape[0]} samples...\")\n",
    "\n",
    "    # Initialize and fit ECOD\n",
    "    model = ECOD(contamination=contamination)\n",
    "    model.fit(X_train.toarray())\n",
    "\n",
    "    # Predict and score\n",
    "    y_pred = model.predict(X_eval.toarray())          # 0 = normal, 1 = anomaly\n",
    "    scores = model.decision_function(X_eval.toarray())  # higher = more normal\n",
    "\n",
    "    # Compute evaluation metrics (anomaly-class focused)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_eval, scores)\n",
    "    except ValueError:\n",
    "        roc_auc = np.nan\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_eval, y_pred, average=None, labels=[1]\n",
    "    )\n",
    "    precision_anom, recall_anom, f1_anom = precision[0], recall[0], f1[0]\n",
    "\n",
    "    # Pack results\n",
    "    result_df = pd.DataFrame({\n",
    "        \"true_label\": y_eval,\n",
    "        \"pred_label\": y_pred,\n",
    "        \"anomaly_score\": scores\n",
    "    })\n",
    "\n",
    "    print(\"\\nâœ… Training complete.\")\n",
    "    print(f\"F1(anomaly): {f1_anom:.4f}, Recall(anomaly): {recall_anom:.4f}, \"\n",
    "          f\"Precision(anomaly): {precision_anom:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    return result_df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eefc4dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ECOD on 390755 normal samples, evaluating on 172519 samples...\n",
      "\n",
      "âœ… Training complete.\n",
      "F1(anomaly): 0.4462, Recall(anomaly): 0.7402, Precision(anomaly): 0.3193, ROC AUC: 0.9573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97    167468\n",
      "           1       0.32      0.74      0.45      5051\n",
      "\n",
      "    accuracy                           0.95    172519\n",
      "   macro avg       0.66      0.85      0.71    172519\n",
      "weighted avg       0.97      0.95      0.96    172519\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and evaluate ECOD\n",
    "result_df, ecod_model = train_and_evaluate_ecod(\n",
    "    X_train=X_train_normal,\n",
    "    X_eval=X_eval,\n",
    "    y_train=y_train,\n",
    "    y_eval=y_eval,\n",
    "    contamination=0.05\n",
    ")\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(result_df[\"true_label\"], result_df[\"pred_label\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andyVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
